{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f67e005",
   "metadata": {},
   "source": [
    "# LSTM Model for Predicting Myers Briggs Personality Types\n",
    "### By Arnav Bhakta$^{1}$ and William Yue$^{1}$\n",
    "#### $^{1}$ Phillips Academy Andover\n",
    "\n",
    "##### The authors would like to thank Patrick Chen, Michael Huang, and Ali Cy for their helpful input and advice in crafting this notebook.\n",
    "\n",
    "In the current notebook, we begin laying the seeds for an LSTM model that will be used to predict Myers Briggs Type Indicators (MBTIs) from an inputted text. As a precursor to this notebook and an introduction to MBTIs and the dataset that we utilize in the current study, please see my [previous notebook](https://github.com/bharnav/CSC630-Machine-Learning/blob/main/Predicting%20Myers%20Briggs%20Types/models/myers-briggs-data_0-lstm.ipynb) on the data pre-processing steps we took, to ensure that the data is optimized for the model it is being passed into.\n",
    "\n",
    "As the purpose of this notebook is to begin to expirement with different models to see which ones provide the most accurate predictions, before settling on a particular framework and constructing a more complex architecture. We start with LSTM, because Hernandez and Knight had previously compared SimpleRNNs, GRUs, and LSTMs, to see which of the specified models performed most optimally when predicting MBTIs [[1]](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6839354.pdf). In specific, for the specific subtype of personality types that we are beginning our MBTI predictions with (Introversion and Extroversion) their LSTM achieved a 54.0% accuracy in classification [[1]](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6839354.pdf). Though these results themselves are very subpar, as LSTMs outperformed all other models tested, we would rather favor it, over the other tested models. Moreover, in comparing LSTMs ability to properly classify short texts (IMBD reviews, Douban comments, PTT posts, etc.), as is done in the current study, Wang et al. found that LSTMs consistently outperformed Naïve Bayes (NB) and Extreme Learning Machines (ELMs) [[2]](https://aclanthology.org/O18-1021.pdf). Hence, we will begin by using LSTMs in our classification task, due to their accuracy in previous studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db606e11",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "We import the below libraries to help us with building our LSTM model. Pandas is primarily used for loading in the data from a csv, and creating DataFrames. NumPy is primarily used for creating arrays and simple arithmetic. Regular expression operations (re) are primarily used for splitting up the samples and removing unwanted or implicative text from the dataset. Tensorflow is primarily used to access and utilize different machine learning (ML) architechtures. Tokenize and Tokenizer are primarily used to split up sentences into smaller units or words called tokens, to helping to understand the text and build the model, by making it easier to understand the meaning of the text, by analyzing it as a sequence of words. Matplotlib.pyplot is used for plotting the data. The remaining libraries are used for importing and exporting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c3336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tokenize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import requests\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6fb38",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "The following cells for the most part are a repeat of the data pre-processing steps we took, that I outlined in my [previous notebook](https://github.com/bharnav/CSC630-Machine-Learning/blob/main/Predicting%20Myers%20Briggs%20Types/models/myers-briggs-data_0-lstm.ipynb). Hence, for more information on the pre-processing steps we took, kindly refer to that notebook, as here, I will only be including details of steps that were not taken in our data pre-processing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de424f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/0.csv')\n",
    "df_text = np.array(df['text'])\n",
    "df_label = np.array(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12313427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = []\n",
    "personality_types = ['intj', 'intp', 'entp', 'entj', 'infj', 'infp', 'enfj', 'enfp', 'istj', 'isfj', 'estj', 'esfj', 'istp', 'isfp', 'estp', 'esfp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08116715",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_text:\n",
    "  text = i.lower()\n",
    "  text = re.sub(r'http\\S+', '', text)\n",
    "  text = re.sub(r'https\\S+', '', text)\n",
    "  text = re.sub(r'@[A-Za-z0-9]+','', text)\n",
    "  for j in personality_types:\n",
    "    text = re.sub(j, '', text)\n",
    "  while True:\n",
    "    before_text = text\n",
    "    text=text.replace('  ', ' ')\n",
    "    if before_text == text:\n",
    "      break\n",
    "  df_user.append(np.array(text.split('|||')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db291853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/2wgdj6_s0rbd4wl3f0gy7kkh0000gq/T/ipykernel_90403/856064120.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  df_user = np.array(df_user)\n"
     ]
    }
   ],
   "source": [
    "df_user = np.array(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa75e1",
   "metadata": {},
   "source": [
    "In addition to reducing the dimensionality of the text samples by flattening it to a 1-dimensional array, as we did in the [previous data pre-processing notebook](https://github.com/bharnav/CSC630-Machine-Learning/blob/main/Predicting%20Myers%20Briggs%20Types/models/myers-briggs-data_0-lstm.ipynb), we also flatten the labels for each corresponding text sample, such that in the flattened array of text, each sample has a corresponding label (0 for Extroversion, 1 for Introversion). By doing so, in training our model, we will be able to provide it with text samples, as well as the corresponding labels for these samples, so that it can start to learn patterns in the text and analyze which particular attributes result in specific MBTIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "177d6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_flattened = []\n",
    "df_label_flattened = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4334549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(df_user):\n",
    "  for j in i:\n",
    "    df_user_flattened.append(j)\n",
    "    df_label_flattened.append(df_label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11073c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_flattened = np.array(df_user_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386bc72",
   "metadata": {},
   "source": [
    "### Exploring the Data Distributions\n",
    "\n",
    "Seeing as we have two specific features, it is important to understand more about their distributions by creating histograms, so that we can learn more about the features themselves. For the labels feature, we will primarily be looking at the split between samples labeled for Extroversion versus samples labeled for Introversion, allowing us to understand which is more prevalent in the current dataset. For the text feature, stored in the `NumPy` array `df_user_flattened`, we will be looking at the distribution of the lengths of each of the samples, in order to understand how much each sample will be padded on average, when padded or truncated to 150 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d3af4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(df_label_flattened, bins='auto')\n",
    "plt.title(\"Distribution of Labeled Data in the Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b680a19",
   "metadata": {},
   "source": [
    "As can be seen from the above histogram, the large majority of labels in the dataset are for Extroversion (0) as opposed to Introversion (1) (roughly a 3 to 1 split). Though it is hard to decieve why this may be, a guess as to why, may be that as the text samples were taken from an online forum, where people are more inclined to be conversational, the majority of samples may have consequently exuded extroversive qualities. In discussing the implications of such an uneven split in the dataset, it could be argued that the set does not provide the model with enough introversive samples to truly be able to pick up on what characteristics of the text make in introversive in nature. However, it is also important to note that there are roughly 100k samples for texts labeled as being introversive, which should theoretically be enough for our model to learn what makes a text introvesive. Hence, we shall not alter the distributions of labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d303090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = []\n",
    "for i in df_user_flattened:\n",
    "  sample_lens = i.split(\" \")\n",
    "  lens.append(len(sample_lens))\n",
    "\n",
    "_ = plt.hist(lens, bins='auto')\n",
    "plt.title(\"Distribution of Lengths of Sample Text in the Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee303c25",
   "metadata": {},
   "source": [
    "In observing the the distribution of the lengths of samples in the dataset, it can be seen that roughly 7.5% have a length close to 38 words. Given that the average tweet is roughly 55 words in length, we can assume that the majority of texts are very short in length and that when padded to a length of 150 words, the sequences will constist largely of 0s. Additionally, in looking at the distribution itself, we can see that it follows a roughly normal distribution, which reminds me of the Central Limit Theorem, and the fact that as we have more samples, as is the case in the current study, the distribution will be more likely to approach an average or most common frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2dc44",
   "metadata": {},
   "source": [
    "### Training, Validation, and Test Splits\n",
    "\n",
    "To train and properly test the accuracy of our model, by defining a training set, validation set, and test set. The training set is the set which used to fit the model and that the model uses to learn patterns in the text of specific categories. Consequently, the training set, consists of both the text samples and its corresponding labels, such that the model can learn how to properly classify texts. The validation set serves a sort a sort of test set, which we will see soon, to test the accuracy of the model while training on a subset of the data. The test set is usde to test how well the model performs when predicting the labels of text, by providing a set of text samples as inputs, having the model predict the corresponding labels of each sample, and comparing it to the actual labels, which were not provided for testing, but were rather kept to compare how well the predicted labels matched the actual labels.\n",
    "\n",
    "We go about splitting up our training, validation, and test sets by attributing 60% of the dataset to the training set, 20% to the validation set, and 20% to the test set (this is done by taking horizontal slices of the dataset). This is done by combining the `df_user_flattened` array of text samples and the `df_label_flattened` array of the corresponding lables, into a single array, `ds`, where column 1 is the actual text samples, and column 2 is the labels that correspond to each of the respective labels. `ds` is then shuffled using `np.random.shuffle` to ensure that there is no specific pattern that the samples in dataset are distributed via, and that they are instead in a random order.\n",
    "\n",
    "As can be seen from `ds_train`, such a process results in the creation of an array that consists of several arrays nested inside of it, which can be observed as containing the sample text in one column, and the label for this text in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cbb4206b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['if i want to enjoy life more then what other types or aspects of type should i try to develop in myself? for instance, should i somehow try to develop more s? in essence what i want is to break...',\n",
       "        0],\n",
       "       [\"i'm not referring to my gender preference. i'm saying that no individual of any gender appeals besides him.\",\n",
       "        0],\n",
       "       [\"i've actually said these exact words once. i'm a daily smoker, which gives some people the idea that i'm lazy, lethargic, and incompetent. but actually, i'm functioning rather fine. i'm going to...\",\n",
       "        0],\n",
       "       ...,\n",
       "       [\"your problem is that you're trying to emulate other types. play the cards you actually have; 's are infamously the most romantic type and it shows. i used to feel the same way, then with time i...\",\n",
       "        0],\n",
       "       [\"i just started a relationship with an girl, and i am . first of all, i love it so far. im used to and so this is a real relief. regarding affection, she's so closed off. if i...\",\n",
       "        0],\n",
       "       ['i’m getting better but gossip used to really bother me. ifyou can’t say something nice about someone, don’t say anything at all.especially with the internet it’s so easy to look up how to interact...',\n",
       "        0]], dtype=object)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 23\n",
    "np.random.seed = seed\n",
    "ds_df = pd.DataFrame()\n",
    "ds_df['posts']=df_user_flattened\n",
    "ds_df['labels'] = df_label_flattened\n",
    "ds = ds_df.to_numpy()\n",
    "np.random.shuffle(ds)\n",
    "front_cutoff = int(ds.shape[0]*0.6)\n",
    "mid_cutoff = int(ds.shape[0]*0.8)\n",
    "ds_train, ds_val, ds_test = ds[:front_cutoff], ds[front_cutoff:mid_cutoff], ds[mid_cutoff:]\n",
    "ds_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ed7d94e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243157, 2)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81e3d4",
   "metadata": {},
   "source": [
    "Next, we split the `ds_train`, `ds_val`, and `ds_test` arrays into two arrays, corresponding to text samples and labels for each array, by taking vertical slices of the data. By doing so, we are able to seperate each array into its predictors or independent variables, and the response from these predictors, or the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "efc85d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_texts, ds_train_labels = ds_train[:,0], ds_train[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "db491f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val_texts, ds_val_labels = ds_val[:, 0], ds_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a04117cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_texts, ds_test_labels = ds_test[:, 0], ds_test[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d597b4",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Once again, I went over tokenization, its importance, and its applications in my [previous data pre-processing notebook](https://github.com/bharnav/CSC630-Machine-Learning/blob/main/Predicting%20Myers%20Briggs%20Types/models/myers-briggs-data_0-lstm.ipynb). As a result, I will not be going over tokenization again, but will instead be filling in details that were not included in the pre-processing notebook. For any questions about tokenization, please refer to that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "043671db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4000 \n",
    "\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size\n",
    "                      ,oov_token= oov_tok\n",
    "                      )\n",
    "tokenizer.fit_on_texts(ds_train_texts)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a9eb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f58fc4",
   "metadata": {},
   "source": [
    "After fitting our tokenizer to the training texts, we save it in a json file, `tokenizer.json`, for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea3c534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer to json\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "59d2162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tokenizer\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d813e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'i': 2,\n",
       " 'the': 3,\n",
       " 'to': 4,\n",
       " 'a': 5,\n",
       " 'and': 6,\n",
       " 'of': 7,\n",
       " 'you': 8,\n",
       " 'that': 9,\n",
       " 'it': 10,\n",
       " 'is': 11,\n",
       " 'in': 12,\n",
       " 'my': 13,\n",
       " 'but': 14,\n",
       " 'for': 15,\n",
       " 'have': 16,\n",
       " 'with': 17,\n",
       " 'me': 18,\n",
       " 'this': 19,\n",
       " \"i'm\": 20,\n",
       " 'be': 21,\n",
       " 'not': 22,\n",
       " 'are': 23,\n",
       " 'like': 24,\n",
       " 'on': 25,\n",
       " 'an': 26,\n",
       " 'as': 27,\n",
       " 'so': 28,\n",
       " 'was': 29,\n",
       " 'if': 30,\n",
       " 'just': 31,\n",
       " 'or': 32,\n",
       " 'do': 33,\n",
       " 'what': 34,\n",
       " 'about': 35,\n",
       " 'think': 36,\n",
       " \"don't\": 37,\n",
       " 'people': 38,\n",
       " 'when': 39,\n",
       " 'your': 40,\n",
       " \"it's\": 41,\n",
       " 'at': 42,\n",
       " 'all': 43,\n",
       " 'can': 44,\n",
       " 'know': 45,\n",
       " 'one': 46,\n",
       " 'they': 47,\n",
       " 'really': 48,\n",
       " 'more': 49,\n",
       " 'would': 50,\n",
       " 'we': 51,\n",
       " 'how': 52,\n",
       " 'out': 53,\n",
       " 'because': 54,\n",
       " 'get': 55,\n",
       " 'am': 56,\n",
       " 'from': 57,\n",
       " \"i've\": 58,\n",
       " 'some': 59,\n",
       " 'time': 60,\n",
       " 'he': 61,\n",
       " 'up': 62,\n",
       " 'very': 63,\n",
       " 'there': 64,\n",
       " 'no': 65,\n",
       " 's': 66,\n",
       " 'them': 67,\n",
       " 'who': 68,\n",
       " 'feel': 69,\n",
       " 'much': 70,\n",
       " 'well': 71,\n",
       " 'being': 72,\n",
       " 'too': 73,\n",
       " 'been': 74,\n",
       " 'by': 75,\n",
       " 'love': 76,\n",
       " 'good': 77,\n",
       " 'things': 78,\n",
       " 'say': 79,\n",
       " 'other': 80,\n",
       " 'want': 81,\n",
       " 'way': 82,\n",
       " 'something': 83,\n",
       " 'see': 84,\n",
       " 'had': 85,\n",
       " 'most': 86,\n",
       " 'will': 87,\n",
       " 'only': 88,\n",
       " 'also': 89,\n",
       " 'then': 90,\n",
       " 'has': 91,\n",
       " 'always': 92,\n",
       " 'she': 93,\n",
       " 'even': 94,\n",
       " 'type': 95,\n",
       " 'than': 96,\n",
       " 'now': 97,\n",
       " 'lot': 98,\n",
       " 'make': 99,\n",
       " 'here': 100,\n",
       " 'her': 101,\n",
       " 'someone': 102,\n",
       " 'their': 103,\n",
       " 'myself': 104,\n",
       " \"you're\": 105,\n",
       " 'life': 106,\n",
       " 'never': 107,\n",
       " 'could': 108,\n",
       " 'go': 109,\n",
       " 'why': 110,\n",
       " 'though': 111,\n",
       " 'any': 112,\n",
       " \"that's\": 113,\n",
       " 'into': 114,\n",
       " 'thing': 115,\n",
       " 'find': 116,\n",
       " 'which': 117,\n",
       " 'actually': 118,\n",
       " \"can't\": 119,\n",
       " 'right': 120,\n",
       " 'first': 121,\n",
       " 'sure': 122,\n",
       " 'going': 123,\n",
       " 'him': 124,\n",
       " 'person': 125,\n",
       " 'yes': 126,\n",
       " 'pretty': 127,\n",
       " 'friends': 128,\n",
       " 'same': 129,\n",
       " 'need': 130,\n",
       " 'were': 131,\n",
       " 'still': 132,\n",
       " 'work': 133,\n",
       " 'his': 134,\n",
       " 'thought': 135,\n",
       " 'said': 136,\n",
       " \"i'd\": 137,\n",
       " 'where': 138,\n",
       " 'over': 139,\n",
       " 'got': 140,\n",
       " 'many': 141,\n",
       " 'did': 142,\n",
       " 'probably': 143,\n",
       " 'read': 144,\n",
       " 'sometimes': 145,\n",
       " '1': 146,\n",
       " 'friend': 147,\n",
       " 'take': 148,\n",
       " 'around': 149,\n",
       " 'thread': 150,\n",
       " 'maybe': 151,\n",
       " 'try': 152,\n",
       " 'off': 153,\n",
       " 'best': 154,\n",
       " 'years': 155,\n",
       " 'back': 156,\n",
       " '2': 157,\n",
       " 'should': 158,\n",
       " 'those': 159,\n",
       " 'anything': 160,\n",
       " \"i'll\": 161,\n",
       " 'our': 162,\n",
       " 'mean': 163,\n",
       " \"doesn't\": 164,\n",
       " 'post': 165,\n",
       " 'little': 166,\n",
       " 'lol': 167,\n",
       " 'does': 168,\n",
       " \"didn't\": 169,\n",
       " 'while': 170,\n",
       " \"'\": 171,\n",
       " 'kind': 172,\n",
       " 'long': 173,\n",
       " 'after': 174,\n",
       " 'few': 175,\n",
       " 'usually': 176,\n",
       " 'better': 177,\n",
       " 'two': 178,\n",
       " '3': 179,\n",
       " 'yeah': 180,\n",
       " 'us': 181,\n",
       " 'these': 182,\n",
       " 'look': 183,\n",
       " 'different': 184,\n",
       " 'day': 185,\n",
       " 'might': 186,\n",
       " 'world': 187,\n",
       " 'ever': 188,\n",
       " 'before': 189,\n",
       " 'makes': 190,\n",
       " 'seems': 191,\n",
       " 'its': 192,\n",
       " 'thanks': 193,\n",
       " 'since': 194,\n",
       " 'understand': 195,\n",
       " 'thinking': 196,\n",
       " 'own': 197,\n",
       " 'may': 198,\n",
       " 'agree': 199,\n",
       " 'others': 200,\n",
       " 'oh': 201,\n",
       " 'hard': 202,\n",
       " 'trying': 203,\n",
       " 'quite': 204,\n",
       " 'both': 205,\n",
       " 'through': 206,\n",
       " 'bit': 207,\n",
       " 'everyone': 208,\n",
       " 'great': 209,\n",
       " 'point': 210,\n",
       " 'mind': 211,\n",
       " 'new': 212,\n",
       " 'come': 213,\n",
       " 'made': 214,\n",
       " 'tell': 215,\n",
       " 'guess': 216,\n",
       " 'every': 217,\n",
       " 'school': 218,\n",
       " 'down': 219,\n",
       " 'talk': 220,\n",
       " 'doing': 221,\n",
       " 'thank': 222,\n",
       " 'interesting': 223,\n",
       " 'use': 224,\n",
       " 'feeling': 225,\n",
       " 'believe': 226,\n",
       " 'types': 227,\n",
       " 'everything': 228,\n",
       " 'happy': 229,\n",
       " 'bad': 230,\n",
       " 'definitely': 231,\n",
       " 'often': 232,\n",
       " 'having': 233,\n",
       " 'used': 234,\n",
       " 'give': 235,\n",
       " 'relationship': 236,\n",
       " \"'s\": 237,\n",
       " 'seem': 238,\n",
       " 'help': 239,\n",
       " 'last': 240,\n",
       " 'guys': 241,\n",
       " 'times': 242,\n",
       " 'personality': 243,\n",
       " 'anyone': 244,\n",
       " 'question': 245,\n",
       " 'another': 246,\n",
       " \"he's\": 247,\n",
       " 'enough': 248,\n",
       " 'part': 249,\n",
       " \"there's\": 250,\n",
       " 'talking': 251,\n",
       " 'hate': 252,\n",
       " 'idea': 253,\n",
       " 'getting': 254,\n",
       " 'such': 255,\n",
       " \"isn't\": 256,\n",
       " 'p': 257,\n",
       " 'true': 258,\n",
       " 'sense': 259,\n",
       " 'tend': 260,\n",
       " 'nice': 261,\n",
       " 'else': 262,\n",
       " 'put': 263,\n",
       " 'once': 264,\n",
       " 'least': 265,\n",
       " 'keep': 266,\n",
       " 'mbti': 267,\n",
       " 'care': 268,\n",
       " 'd': 269,\n",
       " 'sounds': 270,\n",
       " 'either': 271,\n",
       " 'problem': 272,\n",
       " 'self': 273,\n",
       " 'year': 274,\n",
       " 'high': 275,\n",
       " 'music': 276,\n",
       " 'without': 277,\n",
       " 'again': 278,\n",
       " 'sorry': 279,\n",
       " 'fe': 280,\n",
       " 'found': 281,\n",
       " 'nothing': 282,\n",
       " 'yourself': 283,\n",
       " 'haha': 284,\n",
       " 'stuff': 285,\n",
       " 'reading': 286,\n",
       " 'let': 287,\n",
       " 'wrong': 288,\n",
       " 'between': 289,\n",
       " 'especially': 290,\n",
       " 'fun': 291,\n",
       " \"they're\": 292,\n",
       " 'experience': 293,\n",
       " 'however': 294,\n",
       " 'fi': 295,\n",
       " 'reason': 296,\n",
       " 'welcome': 297,\n",
       " 'guy': 298,\n",
       " '5': 299,\n",
       " 'far': 300,\n",
       " 'rather': 301,\n",
       " 'start': 302,\n",
       " '4': 303,\n",
       " 'test': 304,\n",
       " 'ask': 305,\n",
       " 'real': 306,\n",
       " 'forum': 307,\n",
       " 'feelings': 308,\n",
       " 'social': 309,\n",
       " 'looking': 310,\n",
       " 'live': 311,\n",
       " 'done': 312,\n",
       " 'away': 313,\n",
       " 'saying': 314,\n",
       " 'almost': 315,\n",
       " 'each': 316,\n",
       " 'exactly': 317,\n",
       " 'old': 318,\n",
       " 'enjoy': 319,\n",
       " 'remember': 320,\n",
       " 'close': 321,\n",
       " 'ago': 322,\n",
       " 'comes': 323,\n",
       " 'end': 324,\n",
       " 'man': 325,\n",
       " \"wouldn't\": 326,\n",
       " 'yet': 327,\n",
       " 'ni': 328,\n",
       " 'big': 329,\n",
       " 'fact': 330,\n",
       " 'wanted': 331,\n",
       " 'making': 332,\n",
       " 'answer': 333,\n",
       " 'ne': 334,\n",
       " 'already': 335,\n",
       " \"haven't\": 336,\n",
       " 'relate': 337,\n",
       " 'less': 338,\n",
       " 'place': 339,\n",
       " 'girl': 340,\n",
       " 'head': 341,\n",
       " 'change': 342,\n",
       " 'whole': 343,\n",
       " 'able': 344,\n",
       " 'alone': 345,\n",
       " 'functions': 346,\n",
       " 'until': 347,\n",
       " 'im': 348,\n",
       " 'family': 349,\n",
       " 'hope': 350,\n",
       " 'told': 351,\n",
       " 'completely': 352,\n",
       " 'started': 353,\n",
       " 'laughing': 354,\n",
       " 'job': 355,\n",
       " \"we're\": 356,\n",
       " 'interested': 357,\n",
       " 'using': 358,\n",
       " 'cool': 359,\n",
       " 'although': 360,\n",
       " 'show': 361,\n",
       " 'words': 362,\n",
       " 'past': 363,\n",
       " 'certain': 364,\n",
       " \"she's\": 365,\n",
       " 'situation': 366,\n",
       " 'seen': 367,\n",
       " 'felt': 368,\n",
       " 'today': 369,\n",
       " 'wish': 370,\n",
       " 'etc': 371,\n",
       " 'play': 372,\n",
       " 'sort': 373,\n",
       " 'days': 374,\n",
       " 'ti': 375,\n",
       " 'along': 376,\n",
       " 'general': 377,\n",
       " 'emotional': 378,\n",
       " \"wasn't\": 379,\n",
       " 'hey': 380,\n",
       " 'crazy': 381,\n",
       " 'funny': 382,\n",
       " 'mostly': 383,\n",
       " 'thoughts': 384,\n",
       " 'based': 385,\n",
       " 'stop': 386,\n",
       " 'must': 387,\n",
       " 'personally': 388,\n",
       " 'book': 389,\n",
       " 'course': 390,\n",
       " 'god': 391,\n",
       " 'please': 392,\n",
       " 'become': 393,\n",
       " 'took': 394,\n",
       " 'example': 395,\n",
       " 'similar': 396,\n",
       " 'awesome': 397,\n",
       " 'went': 398,\n",
       " 'totally': 399,\n",
       " 'emotions': 400,\n",
       " 'weird': 401,\n",
       " 'night': 402,\n",
       " 'personal': 403,\n",
       " 'strong': 404,\n",
       " \"aren't\": 405,\n",
       " 'ones': 406,\n",
       " 'likely': 407,\n",
       " 'means': 408,\n",
       " 'se': 409,\n",
       " 'matter': 410,\n",
       " 'ok': 411,\n",
       " 'sound': 412,\n",
       " 'questions': 413,\n",
       " 'function': 414,\n",
       " 'favorite': 415,\n",
       " 'met': 416,\n",
       " 'advice': 417,\n",
       " 'important': 418,\n",
       " 'hear': 419,\n",
       " 'open': 420,\n",
       " 'okay': 421,\n",
       " 'recently': 422,\n",
       " 'hi': 423,\n",
       " 'home': 424,\n",
       " 'possible': 425,\n",
       " 'came': 426,\n",
       " 'name': 427,\n",
       " 'dont': 428,\n",
       " 'learn': 429,\n",
       " '6': 430,\n",
       " 'working': 431,\n",
       " 'song': 432,\n",
       " 'couple': 433,\n",
       " 'anyway': 434,\n",
       " 'call': 435,\n",
       " \"what's\": 436,\n",
       " 'whatever': 437,\n",
       " \"won't\": 438,\n",
       " 'together': 439,\n",
       " 'wow': 440,\n",
       " 'te': 441,\n",
       " 'unless': 442,\n",
       " 'months': 443,\n",
       " 'xd': 444,\n",
       " 'taking': 445,\n",
       " 'side': 446,\n",
       " '10': 447,\n",
       " \"you've\": 448,\n",
       " 'women': 449,\n",
       " 'tongue': 450,\n",
       " 'word': 451,\n",
       " 'generally': 452,\n",
       " 'mine': 453,\n",
       " 'seriously': 454,\n",
       " 'mom': 455,\n",
       " 'write': 456,\n",
       " 'face': 457,\n",
       " 'whether': 458,\n",
       " 'female': 459,\n",
       " 'depends': 460,\n",
       " 'writing': 461,\n",
       " 'next': 462,\n",
       " 'second': 463,\n",
       " 'video': 464,\n",
       " 'easy': 465,\n",
       " 'sad': 466,\n",
       " 'consider': 467,\n",
       " 'relationships': 468,\n",
       " 'gets': 469,\n",
       " 'tried': 470,\n",
       " 'shit': 471,\n",
       " 'case': 472,\n",
       " 'honestly': 473,\n",
       " 'watch': 474,\n",
       " 'human': 475,\n",
       " 'small': 476,\n",
       " 'si': 477,\n",
       " 'moment': 478,\n",
       " 'college': 479,\n",
       " 'group': 480,\n",
       " 'perhaps': 481,\n",
       " 'called': 482,\n",
       " 'hello': 483,\n",
       " 'opinion': 484,\n",
       " 'sex': 485,\n",
       " 'conversation': 486,\n",
       " 'e': 487,\n",
       " 'instead': 488,\n",
       " 'ideas': 489,\n",
       " 'game': 490,\n",
       " 'story': 491,\n",
       " 'prefer': 492,\n",
       " 'heard': 493,\n",
       " 'left': 494,\n",
       " 'absolutely': 495,\n",
       " 'says': 496,\n",
       " 'saw': 497,\n",
       " 'share': 498,\n",
       " 'goes': 499,\n",
       " 'age': 500,\n",
       " 'meet': 501,\n",
       " 'free': 502,\n",
       " 'movie': 503,\n",
       " 'feels': 504,\n",
       " 'posts': 505,\n",
       " 'male': 506,\n",
       " 'difficult': 507,\n",
       " 'common': 508,\n",
       " 'heart': 509,\n",
       " 'kinda': 510,\n",
       " 'basically': 511,\n",
       " 't': 512,\n",
       " 'wonder': 513,\n",
       " 'figure': 514,\n",
       " 'introverted': 515,\n",
       " 'eyes': 516,\n",
       " 'money': 517,\n",
       " 'happen': 518,\n",
       " 'deal': 519,\n",
       " 'enneagram': 520,\n",
       " 'watching': 521,\n",
       " 'dad': 522,\n",
       " 'sleep': 523,\n",
       " 'mother': 524,\n",
       " 'explain': 525,\n",
       " 'extremely': 526,\n",
       " 'parents': 527,\n",
       " 'problems': 528,\n",
       " 'asking': 529,\n",
       " 'listen': 530,\n",
       " 'looks': 531,\n",
       " 'happens': 532,\n",
       " 'house': 533,\n",
       " 'o': 534,\n",
       " 'interest': 535,\n",
       " 'asked': 536,\n",
       " 'themselves': 537,\n",
       " 'information': 538,\n",
       " 'realize': 539,\n",
       " '7': 540,\n",
       " 'books': 541,\n",
       " 'fit': 542,\n",
       " 'deep': 543,\n",
       " 'curious': 544,\n",
       " 'stupid': 545,\n",
       " 'simply': 546,\n",
       " 'honest': 547,\n",
       " 'easily': 548,\n",
       " 'coming': 549,\n",
       " 'week': 550,\n",
       " \"couldn't\": 551,\n",
       " 'class': 552,\n",
       " 'seeing': 553,\n",
       " 'hours': 554,\n",
       " 'knew': 555,\n",
       " 'towards': 556,\n",
       " 'theory': 557,\n",
       " 'games': 558,\n",
       " 'act': 559,\n",
       " 'topic': 560,\n",
       " 'three': 561,\n",
       " 'ways': 562,\n",
       " 'order': 563,\n",
       " '8': 564,\n",
       " 'men': 565,\n",
       " 'living': 566,\n",
       " 'understanding': 567,\n",
       " 'serious': 568,\n",
       " 'speak': 569,\n",
       " 'cause': 570,\n",
       " 'noticed': 571,\n",
       " 'posted': 572,\n",
       " 'attention': 573,\n",
       " 'room': 574,\n",
       " 'stay': 575,\n",
       " 'meant': 576,\n",
       " 'child': 577,\n",
       " 'half': 578,\n",
       " 'level': 579,\n",
       " 'dear': 580,\n",
       " 'needs': 581,\n",
       " 'spend': 582,\n",
       " 'full': 583,\n",
       " 'appreciate': 584,\n",
       " 'happened': 585,\n",
       " 'playing': 586,\n",
       " 'hurt': 587,\n",
       " 'beautiful': 588,\n",
       " 'short': 589,\n",
       " 'internet': 590,\n",
       " 'currently': 591,\n",
       " 'sent': 592,\n",
       " '9': 593,\n",
       " 'except': 594,\n",
       " 'hmm': 595,\n",
       " 'works': 596,\n",
       " 'trust': 597,\n",
       " 'perfect': 598,\n",
       " 'loved': 599,\n",
       " 'lack': 600,\n",
       " 'lost': 601,\n",
       " 'hell': 602,\n",
       " 'leave': 603,\n",
       " 'fine': 604,\n",
       " 'proud': 605,\n",
       " 'picture': 606,\n",
       " 'dating': 607,\n",
       " 'given': 608,\n",
       " 'dream': 609,\n",
       " 'f': 610,\n",
       " 'known': 611,\n",
       " 'hand': 612,\n",
       " 'sister': 613,\n",
       " '100': 614,\n",
       " 'list': 615,\n",
       " 'liked': 616,\n",
       " 'art': 617,\n",
       " 'super': 618,\n",
       " 'under': 619,\n",
       " \"you'll\": 620,\n",
       " 'random': 621,\n",
       " 'wait': 622,\n",
       " 'outside': 623,\n",
       " 'amazing': 624,\n",
       " 'move': 625,\n",
       " 'girls': 626,\n",
       " 'glad': 627,\n",
       " 'imagine': 628,\n",
       " 'due': 629,\n",
       " 'during': 630,\n",
       " 'lots': 631,\n",
       " 'n': 632,\n",
       " 'physical': 633,\n",
       " 'learning': 634,\n",
       " 'kids': 635,\n",
       " 'add': 636,\n",
       " 'dark': 637,\n",
       " 'body': 638,\n",
       " 'online': 639,\n",
       " 'brain': 640,\n",
       " 'against': 641,\n",
       " 'takes': 642,\n",
       " 'taken': 643,\n",
       " 'cognitive': 644,\n",
       " 'difference': 645,\n",
       " 'fear': 646,\n",
       " 'brother': 647,\n",
       " 'future': 648,\n",
       " 'nature': 649,\n",
       " 'turn': 650,\n",
       " 'wants': 651,\n",
       " 'view': 652,\n",
       " 'issue': 653,\n",
       " 'issues': 654,\n",
       " 'later': 655,\n",
       " 'woman': 656,\n",
       " 'truth': 657,\n",
       " 'kid': 658,\n",
       " 'cannot': 659,\n",
       " 'death': 660,\n",
       " 'wondering': 661,\n",
       " 'angry': 662,\n",
       " 'run': 663,\n",
       " 'control': 664,\n",
       " 'quiet': 665,\n",
       " \"here's\": 666,\n",
       " 'food': 667,\n",
       " 'confused': 668,\n",
       " 'black': 669,\n",
       " 'reasons': 670,\n",
       " 'eat': 671,\n",
       " 'response': 672,\n",
       " 'inside': 673,\n",
       " 'fall': 674,\n",
       " 'above': 675,\n",
       " 'learned': 676,\n",
       " 'single': 677,\n",
       " 'simple': 678,\n",
       " 'energy': 679,\n",
       " 'sx': 680,\n",
       " 'younger': 681,\n",
       " 'romantic': 682,\n",
       " 'late': 683,\n",
       " 'listening': 684,\n",
       " 'character': 685,\n",
       " 'gonna': 686,\n",
       " 'logic': 687,\n",
       " 'science': 688,\n",
       " 'language': 689,\n",
       " 'young': 690,\n",
       " 'value': 691,\n",
       " 'set': 692,\n",
       " 'normal': 693,\n",
       " 'sp': 694,\n",
       " 'laugh': 695,\n",
       " 'stand': 696,\n",
       " 'process': 697,\n",
       " 'older': 698,\n",
       " 'clear': 699,\n",
       " 'date': 700,\n",
       " 'reality': 701,\n",
       " 'suppose': 702,\n",
       " 'meaning': 703,\n",
       " 'truly': 704,\n",
       " 'cold': 705,\n",
       " 'top': 706,\n",
       " 'quote': 707,\n",
       " 'several': 708,\n",
       " \"'i\": 709,\n",
       " 'aware': 710,\n",
       " 'telling': 711,\n",
       " 'bored': 712,\n",
       " 'tests': 713,\n",
       " 'worth': 714,\n",
       " 'decided': 715,\n",
       " 'fuck': 716,\n",
       " 'focus': 717,\n",
       " 'wink': 718,\n",
       " 'afraid': 719,\n",
       " 'j': 720,\n",
       " 'comfortable': 721,\n",
       " 'non': 722,\n",
       " 'english': 723,\n",
       " 'hair': 724,\n",
       " 'description': 725,\n",
       " 'certainly': 726,\n",
       " 'giving': 727,\n",
       " 'movies': 728,\n",
       " 'system': 729,\n",
       " 'plan': 730,\n",
       " 'state': 731,\n",
       " 'break': 732,\n",
       " 'cute': 733,\n",
       " 'choose': 734,\n",
       " 'doubt': 735,\n",
       " 'across': 736,\n",
       " 'u': 737,\n",
       " 'boyfriend': 738,\n",
       " 'perc': 739,\n",
       " 'anymore': 740,\n",
       " 'early': 741,\n",
       " 'likes': 742,\n",
       " 'damn': 743,\n",
       " 'whenever': 744,\n",
       " 'constantly': 745,\n",
       " 'related': 746,\n",
       " 'anxiety': 747,\n",
       " 'mentioned': 748,\n",
       " 'boring': 749,\n",
       " 'dominant': 750,\n",
       " 'lately': 751,\n",
       " 'rest': 752,\n",
       " 'huge': 753,\n",
       " 'math': 754,\n",
       " 'dreams': 755,\n",
       " 'obviously': 756,\n",
       " 'finally': 757,\n",
       " 'particular': 758,\n",
       " 'number': 759,\n",
       " 'low': 760,\n",
       " 'form': 761,\n",
       " 'space': 762,\n",
       " 'characters': 763,\n",
       " 'society': 764,\n",
       " 'starting': 765,\n",
       " 'check': 766,\n",
       " 'finding': 767,\n",
       " 'typing': 768,\n",
       " 'somewhere': 769,\n",
       " 'speaking': 770,\n",
       " 'avoid': 771,\n",
       " 'logical': 772,\n",
       " 'bring': 773,\n",
       " 'main': 774,\n",
       " 'weeks': 775,\n",
       " 'pain': 776,\n",
       " 'respect': 777,\n",
       " 'pick': 778,\n",
       " \"let's\": 779,\n",
       " 'middle': 780,\n",
       " 'situations': 781,\n",
       " 'notice': 782,\n",
       " 'power': 783,\n",
       " 'results': 784,\n",
       " 'experiences': 785,\n",
       " 'tapatalk': 786,\n",
       " 'eye': 787,\n",
       " 'negative': 788,\n",
       " 'knows': 789,\n",
       " 'specific': 790,\n",
       " 'phone': 791,\n",
       " 'smile': 792,\n",
       " 'term': 793,\n",
       " 'forget': 794,\n",
       " 'computer': 795,\n",
       " 'light': 796,\n",
       " 'introvert': 797,\n",
       " 'describe': 798,\n",
       " 'hold': 799,\n",
       " 'gave': 800,\n",
       " 'realized': 801,\n",
       " 'reply': 802,\n",
       " 'knowledge': 803,\n",
       " 'major': 804,\n",
       " 'considered': 805,\n",
       " 'within': 806,\n",
       " 'knowing': 807,\n",
       " 'shy': 808,\n",
       " 'awkward': 809,\n",
       " 'sensitive': 810,\n",
       " 'behind': 811,\n",
       " 'annoying': 812,\n",
       " 'worry': 813,\n",
       " '20': 814,\n",
       " 'follow': 815,\n",
       " 'helps': 816,\n",
       " 'points': 817,\n",
       " 'extroverted': 818,\n",
       " 'opposite': 819,\n",
       " 'somehow': 820,\n",
       " 'edit': 821,\n",
       " 'mental': 822,\n",
       " 'style': 823,\n",
       " 'op': 824,\n",
       " 'line': 825,\n",
       " 'terms': 826,\n",
       " 'accurate': 827,\n",
       " 'soon': 828,\n",
       " 'party': 829,\n",
       " 'father': 830,\n",
       " 'perspective': 831,\n",
       " 'obvious': 832,\n",
       " 'literally': 833,\n",
       " 'subject': 834,\n",
       " 'ex': 835,\n",
       " 'mood': 836,\n",
       " 'ability': 837,\n",
       " 'wrote': 838,\n",
       " 'strange': 839,\n",
       " 'easier': 840,\n",
       " 'emotionally': 841,\n",
       " 'natural': 842,\n",
       " 'walk': 843,\n",
       " 'somewhat': 844,\n",
       " 'white': 845,\n",
       " 'thats': 846,\n",
       " 'answers': 847,\n",
       " 'entire': 848,\n",
       " 'fucking': 849,\n",
       " 'children': 850,\n",
       " 'site': 851,\n",
       " 'tired': 852,\n",
       " 'special': 853,\n",
       " 'hit': 854,\n",
       " 'forums': 855,\n",
       " 'cry': 856,\n",
       " 'necessarily': 857,\n",
       " 'research': 858,\n",
       " 'trouble': 859,\n",
       " 'dom': 860,\n",
       " 'supposed': 861,\n",
       " 'gone': 862,\n",
       " 'possibly': 863,\n",
       " 'depression': 864,\n",
       " 'behavior': 865,\n",
       " 'study': 866,\n",
       " 'worked': 867,\n",
       " 'rarely': 868,\n",
       " 'wear': 869,\n",
       " 'particularly': 870,\n",
       " 'attracted': 871,\n",
       " 'amount': 872,\n",
       " 'married': 873,\n",
       " 'bed': 874,\n",
       " 'x': 875,\n",
       " 'ah': 876,\n",
       " 'healthy': 877,\n",
       " 'religion': 878,\n",
       " 'worst': 879,\n",
       " 'smart': 880,\n",
       " 'touch': 881,\n",
       " 'fellow': 882,\n",
       " 'chance': 883,\n",
       " 'argument': 884,\n",
       " 'comment': 885,\n",
       " 'songs': 886,\n",
       " 'morning': 887,\n",
       " 'otherwise': 888,\n",
       " 'avatar': 889,\n",
       " 'looked': 890,\n",
       " 'vs': 891,\n",
       " 'apparently': 892,\n",
       " 'gotten': 893,\n",
       " 'inferior': 894,\n",
       " 'values': 895,\n",
       " 'straight': 896,\n",
       " 'intelligence': 897,\n",
       " 'needed': 898,\n",
       " 'die': 899,\n",
       " 'contact': 900,\n",
       " 'shows': 901,\n",
       " 'traits': 902,\n",
       " 'choice': 903,\n",
       " 'expect': 904,\n",
       " 'watched': 905,\n",
       " 'posting': 906,\n",
       " 'degree': 907,\n",
       " 'fairly': 908,\n",
       " 'intuition': 909,\n",
       " 'hot': 910,\n",
       " 'dislike': 911,\n",
       " 'sit': 912,\n",
       " 'sweet': 913,\n",
       " 'threads': 914,\n",
       " 'country': 915,\n",
       " 'blue': 916,\n",
       " 'b': 917,\n",
       " 'spent': 918,\n",
       " 'four': 919,\n",
       " 'voice': 920,\n",
       " 'note': 921,\n",
       " 'conversations': 922,\n",
       " 'gives': 923,\n",
       " 'positive': 924,\n",
       " 'creative': 925,\n",
       " 'tv': 926,\n",
       " 'partner': 927,\n",
       " 'correct': 928,\n",
       " 'cat': 929,\n",
       " 'hug': 930,\n",
       " 'express': 931,\n",
       " 'itself': 932,\n",
       " 'stress': 933,\n",
       " 'career': 934,\n",
       " 'accept': 935,\n",
       " 'depressed': 936,\n",
       " 'worse': 937,\n",
       " 'humor': 938,\n",
       " 'current': 939,\n",
       " 'intuitive': 940,\n",
       " 'result': 941,\n",
       " 'link': 942,\n",
       " 'highly': 943,\n",
       " 'youtube': 944,\n",
       " 'month': 945,\n",
       " 'public': 946,\n",
       " 'clearly': 947,\n",
       " 'car': 948,\n",
       " 'evil': 949,\n",
       " 'unhealthy': 950,\n",
       " 'lives': 951,\n",
       " 'unfortunately': 952,\n",
       " 'seemed': 953,\n",
       " 'history': 954,\n",
       " 'lose': 955,\n",
       " \"people's\": 956,\n",
       " 'fast': 957,\n",
       " 'terrible': 958,\n",
       " 'miss': 959,\n",
       " 'minutes': 960,\n",
       " 'born': 961,\n",
       " 'lazy': 962,\n",
       " 'front': 963,\n",
       " 'step': 964,\n",
       " 'admit': 965,\n",
       " 'series': 966,\n",
       " 'hang': 967,\n",
       " 'assume': 968,\n",
       " 'thinks': 969,\n",
       " 'complete': 970,\n",
       " 'mention': 971,\n",
       " 'psychology': 972,\n",
       " 'drive': 973,\n",
       " 'written': 974,\n",
       " 'dead': 975,\n",
       " '0': 976,\n",
       " 'upon': 977,\n",
       " 'red': 978,\n",
       " 'buy': 979,\n",
       " 'changed': 980,\n",
       " 'friendship': 981,\n",
       " 'text': 982,\n",
       " 'wanting': 983,\n",
       " 'decide': 984,\n",
       " 'putting': 985,\n",
       " 'create': 986,\n",
       " 'unsure': 987,\n",
       " 'coffee': 988,\n",
       " 'wanna': 989,\n",
       " 'opinions': 990,\n",
       " 'fair': 991,\n",
       " 'teacher': 992,\n",
       " 'developed': 993,\n",
       " 'typed': 994,\n",
       " 'longer': 995,\n",
       " 'rules': 996,\n",
       " 'kill': 997,\n",
       " 'useful': 998,\n",
       " 'helpful': 999,\n",
       " 'quickly': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970c947",
   "metadata": {},
   "source": [
    "Much like in the [previous data pre-processing notebook](https://github.com/bharnav/CSC630-Machine-Learning/blob/main/Predicting%20Myers%20Briggs%20Types/models/myers-briggs-data_0-lstm.ipynb), we create sequences of integers, for each of the `tokenized_train`,  `tokenized_val`, and `tokenized_test` arrays, to make the texts interpretable for our model. Once again, if you have any questions about `text_to_sequences`, please refer to the linked notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "756b7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.texts_to_sequences(ds_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09cb9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val = tokenizer.texts_to_sequences(ds_val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1c7d68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(ds_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334dad82",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Padding, the method we use for padding, and the reason behind the `maxlen` we set were all discussed in the [previous data notebook](https://github.com/bharnav/CSC630-Machine-Learning/blob/main/Predicting%20Myers%20Briggs%20Types/models/myers-briggs-data_0-lstm.ipynb). We carry out the same methodology in the current notebook, by padding each of the text samples in the `padded` (`padded_train`), `padded_val`, and `padding_test` sets. For any questions about padding, please refer to the linked notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1481e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(tokenized_train, maxlen=150, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f828b87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243157, 150)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d92d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_val = pad_sequences(tokenized_val, maxlen=150, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3af27686",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = pad_sequences(tokenized_test, maxlen=150, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b66f5",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM Model\n",
    "\n",
    "In the following section, we will be discussing the model utilized in this study. As we are simply testing the effectiveness of different models in classifying sample texts on their MBTIs, we do not begin by constructing an intricate and complex architecture, but instead begin with a very baseline model.\n",
    "\n",
    "We start building out model, by defining a `Sequential` model using `Tensorflow`'s `Keras`: a stack of layers in a neural network, where each layer has one input tensor and one output tensor. The layers of the model as as follows: an embedding layer, a bidirectional LSTM, a dropout layer, a bidirectional LSTM, a dropout layer, a dense layer, a dropout layer, and a final dense layer, as outlined in the model summary.\n",
    "\n",
    "#### Embedding Layers\n",
    "\n",
    "Embedding layers are utilized in neural networks to handle word embedding, or a process by which words are represented via vectors. As opposed to more traditional bag-of-word encoding, which produce very sparce vectors, embedding provides very dense vectors, where each vector represents a single word. As a result, an embedding layer is often used as the first hidden layer in neural networks, when dealing with text data [[3]](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/).\n",
    "\n",
    "As an input, the layer requires an integer representation of the words, which we have already done via sequencing the tokenized text samples. It then requires 3 arguments, being the `input_dim`, the `output_dim`, and the `input_length`. The `input_dim` refers to the vocabulary size, or the the number of words that are represented in the sequenced data, that we defined while tokenizing the sample texts. The `output_dim` refers to the size of the vectors space that the words we pass in, will be embedded into. More generally, it sets the size of the output vector for each word. In the current study, we begin by setting the `output_dim` to 16, however, with tuning and testing, this value is subject to change. The final argument that we specify is the `input_length`, which refers to the number of words in each input sequence. During padding, we padded and truncated all samples to a length of 150 words, so this is the value we specify for this argument [[3]](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/).\n",
    "\n",
    "#### Bidirectional LSTM\n",
    "\n",
    "The following explanation requires a bit of familiarity with LSTMs. If you don't have that, that's okay! You can refer to my explanation of LSTMs in [this notebook](https://github.com/bharnav/CSC630-Machine-Learning/blob/main/pyData/Diabetes%20Dataset%20Investigation.ipynb) and come back here.\n",
    "\n",
    "Bidirectional LSTMs can be viewed as a subtypes of LSTMs that have been improved for classification tasks. It does so by training two LSTMs in parallel on the input data, as opposed to the traditional one, by having one be trained on the forward input sequence and the other being trained on the backward input sequence. This is done by simply duplicating the the first layer of the LSTM in the network, such that it results in there being two layers side by side and have better access to all of the input information that is provided [[4]](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/), [[5]](https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf). In doing so, it is able to split the state of the neurons, to get a more whole representation of the sample texts, as opposed to simply having a linear interpretation [[4]](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/), [[5]](https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf). From a more wholistic view, this can be explained as not being able to understand a sequence of words, until you know what their future context is, emphasizing the need to have a betterunderstanding of the \"end\" of the sequence [[6]](https://www.sciencedirect.com/science/article/pii/S0893608005001206).\n",
    "\n",
    "In this notebook, we utilize 2 bidirectional LSTMs as a baseline to test their effectiveness for our specific classification task. Each layer consists of 16 memory units, with the first hidden layer having a fully connected layer, that utilizes a sigmoid activation function to provide an adequate output.\n",
    "\n",
    "#### Dropout Layer\n",
    "\n",
    "Neural networks are often at risk of overfitting when training, which can have several adverse effects, including becoming accustomed to the noise of the training set, and consequently having low accuracy when predicting on new datasets. Dropout layers seek to prevent overfitting, by randomly ignoring a specificied number of layer outputs (in our case 10%), and allowing the layer to be viewed as a new layer, due to the differing number of nodes and changing connectivity to the input layer. As a result, each layer is seen as being different, meaning that there is a lesser amount of reliance on noise in the training set, when predicting the output vectors, and that there is the opportunity to correct any wrongdoings in previous layers. Thus, dropout layers allow us to place more trust in the fact that our model will be accurate and not overfit [[7]](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).\n",
    "\n",
    "Due to its relaiability, we place dropout layers in between each of the layers we use in this model.\n",
    "\n",
    "#### Dense Layer\n",
    "\n",
    "Dense layers are deeply connected layers in a neural network, that recieves input from the neurons of previous layers, and uses matrix-vector multiplication to transform the vectors into a single vector of a specified size. This specific value is specified via the `units` parameter. As our model consists of two dense layers, the first vector outputted has a size of 8, while the second has a size of 1. In our model, we also specify activation functions for each dense layer, via the `activation` parameter. Activation functions are often used to create a non-linear relationship in the input neuron, such that the model can learn more complex relationships between input and output vectors [[8]](https://machinelearningknowledge.ai/keras-dense-layer-explained-for-beginners/). The activation function we use for the first dense layer is a rectified linear unit  (ReLU), which is a piecewise linear function, that outputs the input directly if the input is positive, or zero otherwise. In doing so, it allows model to learn faster and perform better. A sigmoid activation function is simply a logistic function, that that transforms the input into a value between 0 and 1 [[9]](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/).\n",
    "\n",
    "#### Learning Rate\n",
    "\n",
    "The learning rate is a tuning paramater that specifies the step size at which weights are updated during training, while the loss function as it approaches its minimum. By doing so, it is responsible for how the model changes as a result of the estimated error it produces, each time its weights update, meaning that it is also in charge of how quickly the model adapts to the presented task. A large learning rate can cause the model to converge very quickly in the direction of an incorrect prediction, so for our model, we set a learning rate of 0.0001, so that the model spends enough time understanding trends in the input sample texts and updates accordingly [[10]](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/).\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "A loss function can be viewed as providing a metric for how well your model fits the training set. More specifically, as we are tasked with a binary classification problem, the loss function we utilize is binary cross-entropy:\n",
    "\n",
    "$$H_{p}(q)=-\\frac{1}{N}\\sum_{i=1}^{N}y_{i}\\cdot\\log(p(y_{i}))+(1-y_{i})\\cdot\\log(1-p(y_{i}))$$\n",
    "\n",
    "where $y$ is the label for the corresponding sample text, and $p(y)$ is the probability of the label being 1, for all of the labels in the set. More specifically, for each label of value 1, it adds the log probability of it being 1, $\\log(p(y))$ to the loss, and adds the log probablity of the label being 0, $\\log(1-p(y))$, for each label of value 0. In doing so it is able to provide a justified predicted probability of the classifier being right, while also penalizing the model for inaccurate predictions [[11]](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b796dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 150, 16)           64000     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 150, 32)           4224      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 74,769\n",
      "Trainable params: 74,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(4000, 16, input_length=150),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "learning_rate = 0.0001\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=['accuracy', 'mse', 'mae', 'mape'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea61095",
   "metadata": {},
   "source": [
    "After creating the model, we then train the model, by fitting it to the padded training texts and its corresponding labels. I train the model on 5 epochs – nowhere near to final number of epochs I would train the model on, due to the low learning rate – in order to get a general understanding of how well the bidirectional LSTM performs when predicting the MBTI types. Training for the model took roughly 6-7 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ed313a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30395/30395 [==============================] - 3368s 111ms/step - loss: 0.5511 - accuracy: 0.7669 - mse: 0.1823 - mae: 0.3662 - mape: 192000984.7780 - val_loss: 0.5412 - val_accuracy: 0.7657 - val_mse: 0.1781 - val_mae: 0.3399 - val_mape: 156226560.0000\n",
      "Epoch 2/5\n",
      "30395/30395 [==============================] - 5146s 169ms/step - loss: 0.5379 - accuracy: 0.7659 - mse: 0.1771 - mae: 0.3556 - mape: 181249941.0486 - val_loss: 0.5371 - val_accuracy: 0.7657 - val_mse: 0.1767 - val_mae: 0.3547 - val_mape: 178648384.0000\n",
      "Epoch 3/5\n",
      "30395/30395 [==============================] - 2724s 90ms/step - loss: 0.5305 - accuracy: 0.7684 - mse: 0.1741 - mae: 0.3500 - mape: 178211269.8192 - val_loss: 0.5384 - val_accuracy: 0.7657 - val_mse: 0.1772 - val_mae: 0.3614 - val_mape: 189144192.0000\n",
      "Epoch 4/5\n",
      "30395/30395 [==============================] - 4595s 151ms/step - loss: 0.5303 - accuracy: 0.7655 - mse: 0.1743 - mae: 0.3501 - mape: 177314560.3795 - val_loss: 0.5392 - val_accuracy: 0.7657 - val_mse: 0.1774 - val_mae: 0.3450 - val_mape: 166056624.0000\n",
      "Epoch 5/5\n",
      "30395/30395 [==============================] - 4269s 140ms/step - loss: 0.5279 - accuracy: 0.7657 - mse: 0.1734 - mae: 0.3480 - mape: 175773003.3346 - val_loss: 0.5401 - val_accuracy: 0.7656 - val_mse: 0.1777 - val_mae: 0.3469 - val_mape: 169235728.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc64c2b5fa0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=padded, y=np.asarray(ds_train_labels).astype('float32'), batch_size=8, epochs=5, validation_data=(padded_val, np.asarray(ds_val_labels).astype('float32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736318c0",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The final step we take, is seeing how well the model performs when predicting on the testing set. We do this by using the `model.predict` function, and having it predicting on the padding sample texts, and comparing it to the actual labels for the corresponding texts. We can then compute metrics for the accuracy of the predictions, using the `model.evaluate` function.\n",
    "\n",
    "In doing so, we see that in comparison to Hernandez and Knight's model, which recieved an accuracy of 54.0%, our baseline model greatly outperforms theirs with an accuracy of 76.504%. Though this result may seem to be very good compared to those of previous studies, something that seems off, is the fact that in our dataset we know that roughly 25% of the sample texts have a label of 1, while the remaining ~75% have a label of 0. This raises some suspicions due to the fact that we achieved a nearly 75% accuracy, which might implicate that our model might not be as accurate as we had initially assumed, and that it is just predicting 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "af1ee0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8a49aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22614066\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "666c54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = np.asarray(ds_test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4be9817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 150) for input KerasTensor(type_spec=TensorSpec(shape=(None, 150), dtype=tf.float32, name='embedding_5_input'), name='embedding_5_input', description=\"created by layer 'embedding_5_input'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "2533/2533 [==============================] - 7s 2ms/step - loss: 0.5626 - accuracy: 0.7651 - mse: 0.1869 - mae: 0.4043 - mape: 244507056.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5625528693199158,\n",
       " 0.7651042938232422,\n",
       " 0.18689122796058655,\n",
       " 0.40433382987976074,\n",
       " 244507056.0]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(predictions, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c4e01d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 76.50426264296202%\n"
     ]
    }
   ],
   "source": [
    "# check number of correct predictions\n",
    "count = 0.0\n",
    "correct = 0.0\n",
    "for i in range(len(predictions)):\n",
    "    predict = 0\n",
    "    if predictions[i] < 0.5:\n",
    "        predict = 0\n",
    "    else:\n",
    "        predict = 1\n",
    "    if predict == labels_test[i]:\n",
    "        correct += 1.0\n",
    "    \n",
    "    count += 1.0\n",
    "\n",
    "print('test accuracy: ' + str(correct/count*100) +'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a398f70",
   "metadata": {},
   "source": [
    "To test this hypothesis, we look at how many predictions the model actually outputted were 1s, and of these, how many were correct. As the model doesn't necessarily output integer label predictions 1 and 0, we take all predicted labels $\\geq 0.5$ to be a predicted label of 1, and predicted labels $< 0.5$ to be a predicted label of 0. In this manner, we discover that the model only outputted 125 positive label predictions, in comparison to the nearly 8000 predictions it made, of which only 60 of these predictions were correct. Accordingly, it is safe to say that the accuracy of our model was based on a false premise, and that more fine tuning to the model is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1da8f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive predictions: 125\n",
      "\n",
      "indices of positive predictions:\n",
      "[524, 698, 986, 1367, 2766, 3834, 5183, 5817, 5920, 7801, 7957, 9722, 10273, 10472, 12069, 14150, 15378, 16681, 17795, 17797, 17868, 18603, 19804, 20534, 20835, 21738, 21767, 22083, 22650, 22819, 22965, 23655, 25123, 28069, 28670, 28731, 29747, 30038, 31044, 31954, 32388, 32669, 33716, 33966, 33976, 36077, 36294, 36935, 37216, 38093, 38231, 38922, 39228, 39645, 39902, 40392, 41708, 41935, 43202, 45416, 46262, 47024, 47206, 47322, 47798, 48050, 49093, 49362, 49583, 50414, 50517, 51056, 51280, 51866, 52005, 52233, 53121, 54471, 54909, 55677, 56169, 56598, 57158, 57345, 57727, 58106, 58385, 58782, 59439, 59661, 59838, 59986, 60124, 60479, 61242, 61372, 62517, 63030, 64617, 65396, 66385, 66551, 66896, 68632, 69251, 69961, 70813, 71743, 72373, 72896, 73658, 73665, 74598, 74652, 74836, 75253, 77236, 78181, 78841, 78908, 78925, 79375, 79446, 80285, 80304]\n",
      "\n",
      "indices of correct positive predictions:\n",
      "[524, 986, 2766, 3834, 5817, 7801, 10273, 10472, 12069, 15378, 16681, 20835, 21738, 22819, 23655, 28069, 28670, 28731, 29747, 32669, 33966, 33976, 36077, 36935, 37216, 38922, 39902, 41935, 43202, 46262, 47024, 47322, 47798, 48050, 49583, 50414, 50517, 51056, 52005, 54909, 55677, 56169, 56598, 57158, 58385, 59838, 61242, 61372, 65396, 66385, 66551, 69961, 71743, 72373, 72896, 73658, 74652, 75253, 78181, 80285]\n"
     ]
    }
   ],
   "source": [
    "num_pos = 0\n",
    "index_pos = []\n",
    "index_correct_pos = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] >= 0.5:\n",
    "        num_pos += 1\n",
    "        index_pos.append(i)\n",
    "        if labels_test[i] == 1.0:\n",
    "            index_correct_pos.append(i)\n",
    "            \n",
    "print('number of positive predictions: ' + str(num_pos))\n",
    "print('\\nindices of positive predictions:')\n",
    "print(index_pos)\n",
    "print('\\nindices of correct positive predictions:')\n",
    "print(index_correct_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb12f06",
   "metadata": {},
   "source": [
    "Looking more in-depth at the distribution of the predicted label values via a histogram, we see nearly all of the predictions are $< 0.5$. In fact, the majority of the predicred labels recieved values $< 0.3$, indicating that the model had a tendency to predict high values of 0 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c7628c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(predictions, bins='auto')\n",
    "plt.title(\"Histogram for Prediction Output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6b67f",
   "metadata": {},
   "source": [
    "In seeing these results, it is clear a more complex architecture is required to handle this classificaton task. Moreover, it suggests that bidirectional LSTMs may not be the best approach for handing this problem. Instead, we plan on testing the effectiveness of convolutional neural networks (CNNs), which have historically been know to perform optimally for classification tasks. We are considering the possibility of taking a combined approach using eXtreme gradient boosting (XGBoost) and CNNs, to pretrain the data via a decision tree-based method, before passing the outputs as inputs to a model consistent of CNNs. A final model we may consider in the future, is the use of transformers, due to their prevalence in handling textually based data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
