{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e08eb09",
   "metadata": {},
   "source": [
    "# Data Pre-Processing for BERT Pre-Training\n",
    "### Arnav Bhakta in collaboration with Michael Huang on Album Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289861c",
   "metadata": {},
   "source": [
    "In this notebook, I go over the data pre-processing steps that were taking to prepare for the pre-training of our BERT model that we will be using in to encode data. Namely, the pre-training methods that we will be using are Masked Languange Modeling (MLM) and Next Sentence Prediction (NSP). For a brief overview of these methods, MLM gives our BERT model a sentence in which a few tokens are masked, and asks that it returns to us a completed version of the sentence. In doing so, by pre-training BERT on a large corpus of text, we allow it to learn the linguistic patterns and the context of words in sentences, improving its comprehension of the text being used. On the other hand, NSP allows BERT to understand the longer-term dependencies across sentences. It does this by passing BERT 2 sentences and having it decide if it `IsNextSentence` or is `NotNextSentence`. In doing so, BERT is able to learn the stylistic dependencies between sentences and the flow of the text it is being trained on.\n",
    "\n",
    "Hence, in order to be able to pre-train BERT using these methods, there are several pre-processing steps that we must take, for which the ones that we go over in this notebook, are tokenization and extracting pixel data from images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a16a3",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "We import the below libraries to help us with data pre-processing. `cv2` is the OpenCV python library, that is primarily used in this notebook to load image data from the image files in our dataset. `os` is primarily used for changing and identifying the current directory. `pickle` is primarily used to convert Python objects into byte stream, to store in a file for later use. `matplotlib.pyplot` is used for visualizing and graphing data, to understand its distributions. `tokenize` and `Tokenizer` are primarily used to split up sentences into smaller units or words called tokens, to helping to understand the text and build the model, by making it easier to understand the meaning of the text, by analyzing it as a sequence of words. `pad_sequences` is primarily used to ensure that all sequences in a list have the same length. `numpy` is primarily used for creating arrays and simple arithmetic. `pandas` is primarily used for creating DataFrames. `train_test_split` is primiarily used for splitting up our data into training, testing, and validation sets. `sns` is primarily used for plotting data and making figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef552288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84870d8",
   "metadata": {},
   "source": [
    "## Fetching Data\n",
    "In order to start of pre-processing, we must first get our data. To do this, we primarily use `os.listdir` which allows us to fetch the names of the files or directories inside of other directories. We start by setting our initial path to `./dataset`, which is where our entire dataset is housed. We then loop through all of the directories in that folder, where each directory represents an individual album. Upon looping through these directories, we see that each directory contains 2 types of files: `.txt` and `.jpg`. The `.txt` files contain the lyrics of our songs while the `.jpg` files contain the album covers. Hence, we loop through the contents of the directory to first see if any of the names, contain `.jpg`, for which we then record the path to the image. Then, we go through the remaining files to see if the names contain `.txt`, for which if they do, we record the path, open the file, read the lines, and append them to our `lyric_set` list. We also append the corresponding image data of the album cover using `imread()`, which returns a `300x300x3` array, where the `300x300` represents the size of the image, and the `x3` represents the RGB representation of each individual pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34afd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_set = []\n",
    "pixels_set = []\n",
    "path = \"./dataset\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "for album in dir_list:\n",
    "    try:\n",
    "        path_album = \"./dataset\"\n",
    "        path_album += str(\"/\" + album)\n",
    "        dir_album = os.listdir(path_album)\n",
    "        cover_path = path_album\n",
    "        for img in dir_album:\n",
    "            if \".jpg\" in img:\n",
    "                cover_path += \"/\" + img\n",
    "        for song in dir_album:\n",
    "            if \".txt\" in song:\n",
    "                song_path = path_album + \"/\" + song\n",
    "                with open(song_path) as song_lyrics:\n",
    "                    lines = song_lyrics.readlines()\n",
    "                    lyric_set.append(lines)\n",
    "                cover_pixels = cv2.imread(cover_path)\n",
    "                pixels_set.append(cover_pixels)      \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef4db3",
   "metadata": {},
   "source": [
    "As each song file indicates each new line with a `\\n`, we must remove these characters by removing the last character from each element in `lyric_set`, and removing all empty strings that are remaing as a result of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1c6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_set = [[lyric[:-1] for lyric in lyrics] for lyrics in lyric_set]\n",
    "lyric_set = [[lyric for lyric in lyrics if lyric != ''] for lyrics in lyric_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71932156",
   "metadata": {},
   "source": [
    "Finally, the last step we take in fetching our data, is flattening the `lyric_set` list that contains all of our lyrics to a 1D array, to make the tokenization process easier. We do this by looping through each element or each song in the `lyric_set` list and then looping through each element or lyric in each song, and appending this onto an empty string, seperated by `, `, so that each song is represented as a single string as opposed to a list. Finally, we append this string that now contains the lyrics of a song, onto a new list, `lyric_set_flattened`. Additionally, we remove the last two characters from every element in the `lyric_set_flattened` list, to remove the `, ` that were added to the last lyric of each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960a065f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lyric_set_flattened = []\n",
    "for ind_song in lyric_set:\n",
    "    append_song = \"\"\n",
    "    for ind_lyric in ind_song:\n",
    "        append_song += ind_lyric + \", \"\n",
    "    lyric_set_flattened.append(append_song)  \n",
    "lyric_set_flattened = [lyric[:-2] for lyric in lyric_set_flattened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2f1a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My mind is full bursting over, With all these things I can't remember, Every little single memory reminds me of you, My eyes were weary with all these tears, You left your shadow in my dreams, And all my doubts seemed to disappear when you came along, Flowers melting up into the sky, Hear my heart where our love collides, We hear the songs we found in the times we lost our way, Gentle memories replace our tears, All the love we had is still right here, We hear the songs we found in the time we lost our way, From without words can not describe, What caused the stars to fall deep inside, Every little single memory reminds me of you, Our days are gone lost forever, Reflecting light glistening under water, Naturally this could be everything that seems so unreal, Flowers melting up into the sky, Hear my heart where our love collides, We hear the songs we found in the time we lost our way, Gentle memories replace our tears, All the love we had is still right here, We hear the songs we found in the time we lost our way, Flowers melting up into the sky, Hear my heart where our love collides, We hear the songs we found in the time we lost our way, Gentle memories replace our tears, All the love we had is still right here, We hear the songs we found in the time we lost our way\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyric_set_flattened[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2124d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24692\n",
      "24692\n"
     ]
    }
   ],
   "source": [
    "print(len(pixels_set))\n",
    "print(len(lyric_set_flattened))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f26fe",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The next step of our process, is tokenizing our data. We start by creating an array that contains both our `lyric_set_flattened` and `pixels_set` lists. We then shuffle this array in random order, so that when tokenizing and splitting up our dataset into training, testing, and validation sets, there is no specific order in which our data is split up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177cd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed = seed\n",
    "full_set_lyrics = pd.DataFrame()\n",
    "full_set_lyrics['lyrics'] = lyric_set_flattened\n",
    "full_set_lyrics['albums'] = pixels_set\n",
    "full_set_lyrics = full_set_lyrics.to_numpy()\n",
    "np.random.shuffle(full_set_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015473a4",
   "metadata": {},
   "source": [
    "Subsequently, we split our lyric and image data into training, testing, and validation sets, using the `train_test_split` method. The training set is the set which is used to fit the model and that the model uses to learn patterns in the text. Consequently, the training set, consists of both the text samples and its corresponding labels, such that the model can learn how to properly classify texts. The validation set serves a sort of test set, which we will see soon, to test the accuracy of the model while training on a subset of the data. The test set is usde to test how well the model performs when predicting the labels of text, by providing a set of text samples as inputs, having the model predict the corresponding labels of each sample, and comparing it to the actual labels, which were not provided for testing, but were rather kept to compare how well the predicted labels matched the actual image data.\n",
    "\n",
    "In splitting up our training, testing, and validation sets, we set our test size to 20% of the dataset, and the validation set to 25% of the training set, which is another 20% of the entire dataset. Accordingly, the training set is 60% of the dataset. We also set our `random_state = 1`, so that we can reproduce the same training, testing, and validation sets every time we run our notebook, as opposed to having them as random subsets everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a287c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_data = full_set_lyrics[:, 0]\n",
    "image_data = full_set_lyrics[:, 1]\n",
    "lyric_data_train, lyric_data_test, image_data_train, image_data_test = train_test_split(lyric_data, image_data, \n",
    "                                                                                        test_size = 0.2, random_state=1)\n",
    "lyric_data_train, lyric_data_val, image_data_train, image_data_val = train_test_split(lyric_data_train, image_data_train,\n",
    "                                                                                     test_size = 0.25,  random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383751bf",
   "metadata": {},
   "source": [
    "Next we go ahead and tokenize our data. We do so using `Tokenizer` from `keras`. `Tokenizer` takes in a few parameters, when tokenizing the data, which are as follows: num_words returns the ids of the n most commonly used words in the dataset, where n is the vocab_size we defined as `vocab_size` = 59470, `oov_token` is used to replace out of vocabulary words. We defined the size of our vocabulary by first defining it to be a random value, and the using `tokenizer.word_index`, to see how many words were in our vocabulary. Accordingly, we set this value equal to the our `vocab_size`. We also only based our tokenizer off of our training data, such that we are able to truly test our models accuracy and account for any noise that may be across each of the sets. Post-tokenization, we also called `word_index` from our tokenizer, to get the most commonly used words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2a1a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'the': 2,\n",
       " 'i': 3,\n",
       " 'you': 4,\n",
       " 'and': 5,\n",
       " 'to': 6,\n",
       " 'a': 7,\n",
       " 'me': 8,\n",
       " 'it': 9,\n",
       " 'in': 10,\n",
       " 'my': 11,\n",
       " 'of': 12,\n",
       " 'on': 13,\n",
       " 'your': 14,\n",
       " 'that': 15,\n",
       " \"i'm\": 16,\n",
       " 'all': 17,\n",
       " 'is': 18,\n",
       " 'we': 19,\n",
       " 'for': 20,\n",
       " 'be': 21,\n",
       " \"don't\": 22,\n",
       " 'so': 23,\n",
       " 'but': 24,\n",
       " 'like': 25,\n",
       " 'with': 26,\n",
       " 'know': 27,\n",
       " 'this': 28,\n",
       " 'up': 29,\n",
       " \"it's\": 30,\n",
       " 'no': 31,\n",
       " 'just': 32,\n",
       " 'what': 33,\n",
       " 'when': 34,\n",
       " 'love': 35,\n",
       " 'oh': 36,\n",
       " 'chorus': 37,\n",
       " 'get': 38,\n",
       " 'got': 39,\n",
       " 'verse': 40,\n",
       " 'now': 41,\n",
       " 'do': 42,\n",
       " 'out': 43,\n",
       " 'can': 44,\n",
       " 'down': 45,\n",
       " 'if': 46,\n",
       " 'go': 47,\n",
       " 'they': 48,\n",
       " 'was': 49,\n",
       " 'one': 50,\n",
       " \"you're\": 51,\n",
       " 'see': 52,\n",
       " 'are': 53,\n",
       " 'time': 54,\n",
       " 'not': 55,\n",
       " 'never': 56,\n",
       " 'will': 57,\n",
       " 'yeah': 58,\n",
       " 'from': 59,\n",
       " 'back': 60,\n",
       " 'want': 61,\n",
       " 'at': 62,\n",
       " 'have': 63,\n",
       " \"can't\": 64,\n",
       " 'she': 65,\n",
       " 'baby': 66,\n",
       " 'come': 67,\n",
       " 'let': 68,\n",
       " 'take': 69,\n",
       " 'say': 70,\n",
       " 'way': 71,\n",
       " 'make': 72,\n",
       " 'as': 73,\n",
       " 'how': 74,\n",
       " 'he': 75,\n",
       " 'her': 76,\n",
       " 'here': 77,\n",
       " \"i'll\": 78,\n",
       " 'right': 79,\n",
       " 'there': 80,\n",
       " '1': 81,\n",
       " '2': 82,\n",
       " 'away': 83,\n",
       " \"ain't\": 84,\n",
       " 'by': 85,\n",
       " 'life': 86,\n",
       " 'feel': 87,\n",
       " 'man': 88,\n",
       " 'been': 89,\n",
       " 'gonna': 90,\n",
       " 'where': 91,\n",
       " \"i've\": 92,\n",
       " 'wanna': 93,\n",
       " 'need': 94,\n",
       " 'more': 95,\n",
       " 'tell': 96,\n",
       " 'could': 97,\n",
       " 'day': 98,\n",
       " 'our': 99,\n",
       " 'or': 100,\n",
       " 'some': 101,\n",
       " 'give': 102,\n",
       " 'about': 103,\n",
       " 'night': 104,\n",
       " 'through': 105,\n",
       " 'then': 106,\n",
       " 'well': 107,\n",
       " 'who': 108,\n",
       " \"that's\": 109,\n",
       " 'girl': 110,\n",
       " 'them': 111,\n",
       " 'too': 112,\n",
       " 'said': 113,\n",
       " 'think': 114,\n",
       " 'again': 115,\n",
       " 'keep': 116,\n",
       " 'us': 117,\n",
       " 'still': 118,\n",
       " 'la': 119,\n",
       " 'heart': 120,\n",
       " 'only': 121,\n",
       " 'why': 122,\n",
       " 'world': 123,\n",
       " 'cause': 124,\n",
       " 'good': 125,\n",
       " 'his': 126,\n",
       " 'over': 127,\n",
       " 'little': 128,\n",
       " \"there's\": 129,\n",
       " \"won't\": 130,\n",
       " 'around': 131,\n",
       " 'had': 132,\n",
       " 'off': 133,\n",
       " 'hey': 134,\n",
       " 'every': 135,\n",
       " 'eyes': 136,\n",
       " 'look': 137,\n",
       " \"we're\": 138,\n",
       " 'home': 139,\n",
       " 'long': 140,\n",
       " 'am': 141,\n",
       " 'nothing': 142,\n",
       " 'would': 143,\n",
       " 'mind': 144,\n",
       " 'always': 145,\n",
       " 'hook': 146,\n",
       " 'gone': 147,\n",
       " 'better': 148,\n",
       " 'an': 149,\n",
       " \"'cause\": 150,\n",
       " 'find': 151,\n",
       " 'ever': 152,\n",
       " 'these': 153,\n",
       " 'into': 154,\n",
       " 'were': 155,\n",
       " 'things': 156,\n",
       " 'gotta': 157,\n",
       " 'ya': 158,\n",
       " 'head': 159,\n",
       " 'going': 160,\n",
       " 'put': 161,\n",
       " 'hold': 162,\n",
       " 'something': 163,\n",
       " 'nigga': 164,\n",
       " 'than': 165,\n",
       " '3': 166,\n",
       " 'hear': 167,\n",
       " 'before': 168,\n",
       " 'really': 169,\n",
       " 'shit': 170,\n",
       " 'everything': 171,\n",
       " 'try': 172,\n",
       " 'alone': 173,\n",
       " 'tonight': 174,\n",
       " 'him': 175,\n",
       " 'new': 176,\n",
       " 'light': 177,\n",
       " 'stop': 178,\n",
       " 'did': 179,\n",
       " 'much': 180,\n",
       " 'thing': 181,\n",
       " 'call': 182,\n",
       " 'run': 183,\n",
       " 'left': 184,\n",
       " 'hard': 185,\n",
       " 'live': 186,\n",
       " 'leave': 187,\n",
       " 'bridge': 188,\n",
       " 'last': 189,\n",
       " 'turn': 190,\n",
       " 'made': 191,\n",
       " 'face': 192,\n",
       " 'stay': 193,\n",
       " \"you've\": 194,\n",
       " \"she's\": 195,\n",
       " 'another': 196,\n",
       " 'even': 197,\n",
       " 'fuck': 198,\n",
       " 'ooh': 199,\n",
       " 'inside': 200,\n",
       " \"you'll\": 201,\n",
       " 'their': 202,\n",
       " 'niggas': 203,\n",
       " 'fall': 204,\n",
       " 'believe': 205,\n",
       " 'people': 206,\n",
       " 'real': 207,\n",
       " 'old': 208,\n",
       " 'end': 209,\n",
       " 'boy': 210,\n",
       " 'show': 211,\n",
       " 'place': 212,\n",
       " 'break': 213,\n",
       " 'die': 214,\n",
       " 'same': 215,\n",
       " 'has': 216,\n",
       " 'outro': 217,\n",
       " 'lost': 218,\n",
       " 'big': 219,\n",
       " 'two': 220,\n",
       " 'sun': 221,\n",
       " 'god': 222,\n",
       " 'wrong': 223,\n",
       " \"let's\": 224,\n",
       " 'black': 225,\n",
       " 'while': 226,\n",
       " 'done': 227,\n",
       " \"i'd\": 228,\n",
       " 'coming': 229,\n",
       " 'should': 230,\n",
       " \"'em\": 231,\n",
       " 'walk': 232,\n",
       " 'please': 233,\n",
       " 'hand': 234,\n",
       " 'own': 235,\n",
       " 'name': 236,\n",
       " 'bad': 237,\n",
       " 'money': 238,\n",
       " 'without': 239,\n",
       " 'thought': 240,\n",
       " 'rock': 241,\n",
       " 'play': 242,\n",
       " 'yes': 243,\n",
       " 'high': 244,\n",
       " 'hands': 245,\n",
       " 'free': 246,\n",
       " 'change': 247,\n",
       " 'da': 248,\n",
       " 'pre': 249,\n",
       " 'fire': 250,\n",
       " 'na': 251,\n",
       " 'mine': 252,\n",
       " 'other': 253,\n",
       " 'dead': 254,\n",
       " 'wait': 255,\n",
       " \"didn't\": 256,\n",
       " 'myself': 257,\n",
       " 'stand': 258,\n",
       " 'because': 259,\n",
       " 'cold': 260,\n",
       " 'bring': 261,\n",
       " 'hit': 262,\n",
       " 'cry': 263,\n",
       " 'soul': 264,\n",
       " 'bitch': 265,\n",
       " 'ride': 266,\n",
       " 'might': 267,\n",
       " 'side': 268,\n",
       " 'maybe': 269,\n",
       " 'watch': 270,\n",
       " 'everybody': 271,\n",
       " 'looking': 272,\n",
       " 'waiting': 273,\n",
       " 'care': 274,\n",
       " 'i’m': 275,\n",
       " 'ah': 276,\n",
       " 'alright': 277,\n",
       " 'remember': 278,\n",
       " 'enough': 279,\n",
       " 'feeling': 280,\n",
       " \"what's\": 281,\n",
       " 'move': 282,\n",
       " 'must': 283,\n",
       " 'pain': 284,\n",
       " 'song': 285,\n",
       " \"he's\": 286,\n",
       " \"we'll\": 287,\n",
       " 'far': 288,\n",
       " 'came': 289,\n",
       " 'yo': 290,\n",
       " 'first': 291,\n",
       " 'sky': 292,\n",
       " 'start': 293,\n",
       " 'talk': 294,\n",
       " 'those': 295,\n",
       " 'dream': 296,\n",
       " 'days': 297,\n",
       " 'sweet': 298,\n",
       " 'sing': 299,\n",
       " 'until': 300,\n",
       " 'best': 301,\n",
       " 'yourself': 302,\n",
       " 'comes': 303,\n",
       " 'someone': 304,\n",
       " 'hope': 305,\n",
       " 'told': 306,\n",
       " 'hell': 307,\n",
       " 'words': 308,\n",
       " 'forever': 309,\n",
       " 'used': 310,\n",
       " 'young': 311,\n",
       " 'uh': 312,\n",
       " 'trying': 313,\n",
       " 'getting': 314,\n",
       " 'found': 315,\n",
       " 'together': 316,\n",
       " 'knew': 317,\n",
       " 'its': 318,\n",
       " 'many': 319,\n",
       " 'behind': 320,\n",
       " 'open': 321,\n",
       " 'seen': 322,\n",
       " 'music': 323,\n",
       " 'blue': 324,\n",
       " 'true': 325,\n",
       " \"y'all\": 326,\n",
       " 'friends': 327,\n",
       " 'mean': 328,\n",
       " 'door': 329,\n",
       " 'sleep': 330,\n",
       " 'house': 331,\n",
       " 'line': 332,\n",
       " 'dance': 333,\n",
       " 'fight': 334,\n",
       " 'close': 335,\n",
       " 'ready': 336,\n",
       " 'game': 337,\n",
       " 'once': 338,\n",
       " 'town': 339,\n",
       " 'took': 340,\n",
       " 'help': 341,\n",
       " 'crazy': 342,\n",
       " 'work': 343,\n",
       " 'blood': 344,\n",
       " 'roll': 345,\n",
       " 'sound': 346,\n",
       " 'lie': 347,\n",
       " \"they're\": 348,\n",
       " 'body': 349,\n",
       " 'white': 350,\n",
       " 'intro': 351,\n",
       " 'hate': 352,\n",
       " 'dreams': 353,\n",
       " 'wish': 354,\n",
       " 'goes': 355,\n",
       " 'living': 356,\n",
       " 'times': 357,\n",
       " 'forget': 358,\n",
       " 'lose': 359,\n",
       " 'running': 360,\n",
       " 'beat': 361,\n",
       " 'friend': 362,\n",
       " 'heard': 363,\n",
       " 'under': 364,\n",
       " 'fly': 365,\n",
       " 'any': 366,\n",
       " 'deep': 367,\n",
       " 'morning': 368,\n",
       " 'anything': 369,\n",
       " 'rain': 370,\n",
       " 'after': 371,\n",
       " 'lord': 372,\n",
       " 'ground': 373,\n",
       " 'went': 374,\n",
       " 'road': 375,\n",
       " 'whole': 376,\n",
       " 'alive': 377,\n",
       " 'girls': 378,\n",
       " 'sure': 379,\n",
       " 'ass': 380,\n",
       " 'easy': 381,\n",
       " 'dark': 382,\n",
       " 'today': 383,\n",
       " 'makes': 384,\n",
       " 'though': 385,\n",
       " 'burn': 386,\n",
       " 'don’t': 387,\n",
       " 'nobody': 388,\n",
       " 'may': 389,\n",
       " 'lies': 390,\n",
       " 'truth': 391,\n",
       " 'years': 392,\n",
       " 'city': 393,\n",
       " 'hot': 394,\n",
       " 'set': 395,\n",
       " 'street': 396,\n",
       " 'broken': 397,\n",
       " 'o': 398,\n",
       " 'touch': 399,\n",
       " 'kiss': 400,\n",
       " 'sometimes': 401,\n",
       " 'guess': 402,\n",
       " 'listen': 403,\n",
       " 'shake': 404,\n",
       " 'matter': 405,\n",
       " 'lonely': 406,\n",
       " 'woman': 407,\n",
       " 'knows': 408,\n",
       " 'slow': 409,\n",
       " 'save': 410,\n",
       " \"we've\": 411,\n",
       " 'goodbye': 412,\n",
       " 'lot': 413,\n",
       " 'ask': 414,\n",
       " 'damn': 415,\n",
       " 'else': 416,\n",
       " 'somebody': 417,\n",
       " 'room': 418,\n",
       " 'saw': 419,\n",
       " 'fine': 420,\n",
       " 'late': 421,\n",
       " 'red': 422,\n",
       " 'boys': 423,\n",
       " \"'til\": 424,\n",
       " 'smile': 425,\n",
       " 'kill': 426,\n",
       " 'next': 427,\n",
       " 'death': 428,\n",
       " 'round': 429,\n",
       " 'pretty': 430,\n",
       " 'lights': 431,\n",
       " 'kind': 432,\n",
       " 'does': 433,\n",
       " 'word': 434,\n",
       " 'tried': 435,\n",
       " 'falling': 436,\n",
       " 'understand': 437,\n",
       " 'fear': 438,\n",
       " 'wonder': 439,\n",
       " 'hide': 440,\n",
       " 'feet': 441,\n",
       " 'till': 442,\n",
       " 'tears': 443,\n",
       " 'along': 444,\n",
       " 'ha': 445,\n",
       " 'wind': 446,\n",
       " 'wake': 447,\n",
       " 'wanted': 448,\n",
       " 'whoa': 449,\n",
       " 'miss': 450,\n",
       " '4': 451,\n",
       " 'lay': 452,\n",
       " 'war': 453,\n",
       " 'floor': 454,\n",
       " 'water': 455,\n",
       " 'talking': 456,\n",
       " 'it’s': 457,\n",
       " 'air': 458,\n",
       " 'meet': 459,\n",
       " 'happy': 460,\n",
       " 'bed': 461,\n",
       " 'seems': 462,\n",
       " 'past': 463,\n",
       " 'party': 464,\n",
       " 'gave': 465,\n",
       " 'full': 466,\n",
       " 'hurt': 467,\n",
       " 'chance': 468,\n",
       " 'since': 469,\n",
       " 'streets': 470,\n",
       " 'born': 471,\n",
       " \"couldn't\": 472,\n",
       " 'feels': 473,\n",
       " \"gon'\": 474,\n",
       " 'soon': 475,\n",
       " 'b': 476,\n",
       " 'three': 477,\n",
       " 'doing': 478,\n",
       " 'blow': 479,\n",
       " 'child': 480,\n",
       " 'very': 481,\n",
       " 'pull': 482,\n",
       " 'drop': 483,\n",
       " 'de': 484,\n",
       " 'shine': 485,\n",
       " 'strong': 486,\n",
       " \"'bout\": 487,\n",
       " 'between': 488,\n",
       " 'beautiful': 489,\n",
       " 'straight': 490,\n",
       " 'car': 491,\n",
       " 'arms': 492,\n",
       " 'x2': 493,\n",
       " 'such': 494,\n",
       " 'thinking': 495,\n",
       " 'cut': 496,\n",
       " 'each': 497,\n",
       " 'g': 498,\n",
       " 'rest': 499,\n",
       " 'heaven': 500,\n",
       " 'part': 501,\n",
       " 'honey': 502,\n",
       " 'says': 503,\n",
       " 'follow': 504,\n",
       " 'sick': 505,\n",
       " 'control': 506,\n",
       " 'reason': 507,\n",
       " 'fucking': 508,\n",
       " 'top': 509,\n",
       " 'huh': 510,\n",
       " 'making': 511,\n",
       " 'rise': 512,\n",
       " \"who's\": 513,\n",
       " 'throw': 514,\n",
       " 'stars': 515,\n",
       " 'fast': 516,\n",
       " 'p': 517,\n",
       " 'tomorrow': 518,\n",
       " 'step': 519,\n",
       " 'being': 520,\n",
       " 'wall': 521,\n",
       " 'catch': 522,\n",
       " 'gets': 523,\n",
       " \"you'd\": 524,\n",
       " 'eye': 525,\n",
       " 'tired': 526,\n",
       " 'mama': 527,\n",
       " 'everyone': 528,\n",
       " 'cool': 529,\n",
       " 'felt': 530,\n",
       " 'sea': 531,\n",
       " 'pay': 532,\n",
       " 'son': 533,\n",
       " 'breath': 534,\n",
       " 'que': 535,\n",
       " 'standing': 536,\n",
       " 'fool': 537,\n",
       " 'saying': 538,\n",
       " 'check': 539,\n",
       " 'kids': 540,\n",
       " 'burning': 541,\n",
       " 'walking': 542,\n",
       " 'gun': 543,\n",
       " 'seem': 544,\n",
       " 'drink': 545,\n",
       " \"doesn't\": 546,\n",
       " 'front': 547,\n",
       " \"wasn't\": 548,\n",
       " 'anymore': 549,\n",
       " 'apart': 550,\n",
       " 'both': 551,\n",
       " 'low': 552,\n",
       " 'sit': 553,\n",
       " 'use': 554,\n",
       " 'instrumental': 555,\n",
       " 'skin': 556,\n",
       " 'lives': 557,\n",
       " 'sad': 558,\n",
       " 'taking': 559,\n",
       " 'd': 560,\n",
       " 'upon': 561,\n",
       " 'already': 562,\n",
       " 'hair': 563,\n",
       " 'moon': 564,\n",
       " 'speak': 565,\n",
       " 'okay': 566,\n",
       " 'mouth': 567,\n",
       " 'land': 568,\n",
       " 'fun': 569,\n",
       " 't': 570,\n",
       " 'dem': 571,\n",
       " 'yet': 572,\n",
       " 'smoke': 573,\n",
       " \"wouldn't\": 574,\n",
       " 'calling': 575,\n",
       " 'power': 576,\n",
       " 'half': 577,\n",
       " 'mi': 578,\n",
       " 'hood': 579,\n",
       " 'breathe': 580,\n",
       " 'above': 581,\n",
       " 'e': 582,\n",
       " 'caught': 583,\n",
       " 'voice': 584,\n",
       " 'turned': 585,\n",
       " 'men': 586,\n",
       " 'mother': 587,\n",
       " 'afraid': 588,\n",
       " 'great': 589,\n",
       " 'gold': 590,\n",
       " 'di': 591,\n",
       " \"goin'\": 592,\n",
       " 'takes': 593,\n",
       " 'bit': 594,\n",
       " 'empty': 595,\n",
       " 'club': 596,\n",
       " 'king': 597,\n",
       " 'learn': 598,\n",
       " 'cannot': 599,\n",
       " 'moving': 600,\n",
       " 'which': 601,\n",
       " 'perfect': 602,\n",
       " 'buy': 603,\n",
       " 'four': 604,\n",
       " 'train': 605,\n",
       " 'pass': 606,\n",
       " 'drive': 607,\n",
       " 'whatever': 608,\n",
       " 'wild': 609,\n",
       " 'started': 610,\n",
       " 'against': 611,\n",
       " 'different': 612,\n",
       " 'clear': 613,\n",
       " 'most': 614,\n",
       " 'wants': 615,\n",
       " 'dirty': 616,\n",
       " 'carry': 617,\n",
       " 'phone': 618,\n",
       " 'blind': 619,\n",
       " 'five': 620,\n",
       " 'jesus': 621,\n",
       " 'shot': 622,\n",
       " 'daddy': 623,\n",
       " 'hang': 624,\n",
       " 'leaving': 625,\n",
       " 'sorry': 626,\n",
       " 'moment': 627,\n",
       " \"i'mma\": 628,\n",
       " 'u': 629,\n",
       " 'known': 630,\n",
       " 'yours': 631,\n",
       " 'tight': 632,\n",
       " 'mr': 633,\n",
       " 'holding': 634,\n",
       " 'kid': 635,\n",
       " 'you’re': 636,\n",
       " 'star': 637,\n",
       " 'nice': 638,\n",
       " 'mad': 639,\n",
       " 'story': 640,\n",
       " 'across': 641,\n",
       " 'wit': 642,\n",
       " 'jump': 643,\n",
       " 'earth': 644,\n",
       " 'loved': 645,\n",
       " 'babe': 646,\n",
       " 'hearts': 647,\n",
       " 'c': 648,\n",
       " 'ones': 649,\n",
       " 'dog': 650,\n",
       " 'watching': 651,\n",
       " 'trust': 652,\n",
       " 'dying': 653,\n",
       " 'outside': 654,\n",
       " 'lady': 655,\n",
       " 'worth': 656,\n",
       " 'children': 657,\n",
       " 'tear': 658,\n",
       " 'send': 659,\n",
       " 'bitches': 660,\n",
       " 'loving': 661,\n",
       " 'school': 662,\n",
       " 'y': 663,\n",
       " 'dancing': 664,\n",
       " 'blame': 665,\n",
       " 'pray': 666,\n",
       " 'bye': 667,\n",
       " 'peace': 668,\n",
       " 'working': 669,\n",
       " \"isn't\": 670,\n",
       " 'push': 671,\n",
       " 'year': 672,\n",
       " 'brother': 673,\n",
       " 'become': 674,\n",
       " 'lips': 675,\n",
       " 'nights': 676,\n",
       " 'somewhere': 677,\n",
       " 'knees': 678,\n",
       " 'finally': 679,\n",
       " \"c'mon\": 680,\n",
       " 'broke': 681,\n",
       " 'singing': 682,\n",
       " 'radio': 683,\n",
       " 'eat': 684,\n",
       " 'dear': 685,\n",
       " 'met': 686,\n",
       " 'nowhere': 687,\n",
       " 'scared': 688,\n",
       " 'called': 689,\n",
       " 'grow': 690,\n",
       " 's': 691,\n",
       " 'darkness': 692,\n",
       " 'ways': 693,\n",
       " 'looked': 694,\n",
       " 'meant': 695,\n",
       " 'keeps': 696,\n",
       " 'future': 697,\n",
       " 'shoes': 698,\n",
       " 'read': 699,\n",
       " 'taste': 700,\n",
       " 'thank': 701,\n",
       " 'el': 702,\n",
       " 'clothes': 703,\n",
       " 'pick': 704,\n",
       " 'block': 705,\n",
       " 'ice': 706,\n",
       " 'second': 707,\n",
       " 'fade': 708,\n",
       " 'clean': 709,\n",
       " 'green': 710,\n",
       " 'fell': 711,\n",
       " 'window': 712,\n",
       " 'gimme': 713,\n",
       " 'songs': 714,\n",
       " 'lead': 715,\n",
       " 'crying': 716,\n",
       " 'doo': 717,\n",
       " \"nothin'\": 718,\n",
       " 'lover': 719,\n",
       " 'reach': 720,\n",
       " 'wear': 721,\n",
       " 'stone': 722,\n",
       " 'spend': 723,\n",
       " 'solo': 724,\n",
       " 'kick': 725,\n",
       " 'trouble': 726,\n",
       " 'brain': 727,\n",
       " 'm': 728,\n",
       " 'six': 729,\n",
       " 'playing': 730,\n",
       " 'father': 731,\n",
       " 'win': 732,\n",
       " 'miles': 733,\n",
       " 'promise': 734,\n",
       " 'breaking': 735,\n",
       " 'ho': 736,\n",
       " 'ba': 737,\n",
       " 'changed': 738,\n",
       " 'woah': 739,\n",
       " 'act': 740,\n",
       " 'thousand': 741,\n",
       " 'river': 742,\n",
       " 'anyone': 743,\n",
       " 'near': 744,\n",
       " 'pop': 745,\n",
       " 'walls': 746,\n",
       " 'number': 747,\n",
       " 'faith': 748,\n",
       " \"i'ma\": 749,\n",
       " 'sign': 750,\n",
       " 'loud': 751,\n",
       " 'ten': 752,\n",
       " 'worry': 753,\n",
       " 'ring': 754,\n",
       " '5': 755,\n",
       " 'guitar': 756,\n",
       " 'scream': 757,\n",
       " 'shoot': 758,\n",
       " 'summer': 759,\n",
       " 'telling': 760,\n",
       " 'cross': 761,\n",
       " 'te': 762,\n",
       " 'boss': 763,\n",
       " 'piece': 764,\n",
       " 'safe': 765,\n",
       " 'bone': 766,\n",
       " 'million': 767,\n",
       " 'hole': 768,\n",
       " 'waste': 769,\n",
       " 'hoes': 770,\n",
       " 'can’t': 771,\n",
       " 'ay': 772,\n",
       " 'sin': 773,\n",
       " 'darling': 774,\n",
       " 'bright': 775,\n",
       " 'everyday': 776,\n",
       " 'shame': 777,\n",
       " 'everywhere': 778,\n",
       " 'john': 779,\n",
       " 'repeat': 780,\n",
       " 'ahead': 781,\n",
       " 'laugh': 782,\n",
       " 'losing': 783,\n",
       " \"they'll\": 784,\n",
       " 'loves': 785,\n",
       " 'heat': 786,\n",
       " 'thoughts': 787,\n",
       " 'anyway': 788,\n",
       " 'flow': 789,\n",
       " 'fill': 790,\n",
       " 'bird': 791,\n",
       " 'boom': 792,\n",
       " 'turning': 793,\n",
       " 'write': 794,\n",
       " 'shut': 795,\n",
       " 'pussy': 796,\n",
       " 'shout': 797,\n",
       " 'stick': 798,\n",
       " 'rap': 799,\n",
       " 'looks': 800,\n",
       " 'oooh': 801,\n",
       " 'minute': 802,\n",
       " 'york': 803,\n",
       " 'doubt': 804,\n",
       " 'lying': 805,\n",
       " \"talkin'\": 806,\n",
       " 'crack': 807,\n",
       " 'point': 808,\n",
       " 'few': 809,\n",
       " 'mess': 810,\n",
       " 'j': 811,\n",
       " 'middle': 812,\n",
       " 'bang': 813,\n",
       " 'dick': 814,\n",
       " 'picture': 815,\n",
       " 'refrain': 816,\n",
       " 'heavy': 817,\n",
       " 'ocean': 818,\n",
       " 'west': 819,\n",
       " 'count': 820,\n",
       " 'paper': 821,\n",
       " 'hello': 822,\n",
       " 'died': 823,\n",
       " 'within': 824,\n",
       " 'sex': 825,\n",
       " 'walked': 826,\n",
       " 'chain': 827,\n",
       " 'n': 828,\n",
       " 'guy': 829,\n",
       " 'mike': 830,\n",
       " 'space': 831,\n",
       " 'plan': 832,\n",
       " 'giving': 833,\n",
       " 'tryna': 834,\n",
       " 'poor': 835,\n",
       " 'family': 836,\n",
       " 'corner': 837,\n",
       " 'blues': 838,\n",
       " 'memories': 839,\n",
       " 'single': 840,\n",
       " 'strange': 841,\n",
       " 'devil': 842,\n",
       " 'swear': 843,\n",
       " 'stuck': 844,\n",
       " 'share': 845,\n",
       " 'belong': 846,\n",
       " 'memory': 847,\n",
       " \"lookin'\": 848,\n",
       " 'hundred': 849,\n",
       " 'sense': 850,\n",
       " 'cuz': 851,\n",
       " 'angels': 852,\n",
       " 'band': 853,\n",
       " 'guns': 854,\n",
       " 'ran': 855,\n",
       " 'fake': 856,\n",
       " 'angel': 857,\n",
       " \"gettin'\": 858,\n",
       " 'warm': 859,\n",
       " 'deal': 860,\n",
       " \"an'\": 861,\n",
       " 'wine': 862,\n",
       " 'funny': 863,\n",
       " 'wide': 864,\n",
       " 'country': 865,\n",
       " 'glass': 866,\n",
       " 'du': 867,\n",
       " 'silence': 868,\n",
       " 'bout': 869,\n",
       " 'yea': 870,\n",
       " 'state': 871,\n",
       " 'less': 872,\n",
       " 'bet': 873,\n",
       " 'lines': 874,\n",
       " 'sexy': 875,\n",
       " 'cars': 876,\n",
       " 'sight': 877,\n",
       " 'track': 878,\n",
       " 'post': 879,\n",
       " 'higher': 880,\n",
       " \"she'll\": 881,\n",
       " 'closer': 882,\n",
       " 'storm': 883,\n",
       " 'begin': 884,\n",
       " 'style': 885,\n",
       " 'killing': 886,\n",
       " 'r': 887,\n",
       " 'spit': 888,\n",
       " 'im': 889,\n",
       " 'lean': 890,\n",
       " 'fact': 891,\n",
       " 'quick': 892,\n",
       " 'pretend': 893,\n",
       " 'needed': 894,\n",
       " 'happen': 895,\n",
       " 'lo': 896,\n",
       " 'ma': 897,\n",
       " 'l': 898,\n",
       " 'almost': 899,\n",
       " 'queen': 900,\n",
       " \"comin'\": 901,\n",
       " 'crowd': 902,\n",
       " 'dry': 903,\n",
       " 'news': 904,\n",
       " 'diamonds': 905,\n",
       " 'longer': 906,\n",
       " 'bones': 907,\n",
       " 'secret': 908,\n",
       " 'someday': 909,\n",
       " 'answer': 910,\n",
       " 'self': 911,\n",
       " 'ohh': 912,\n",
       " 'dust': 913,\n",
       " 'bleed': 914,\n",
       " 'choose': 915,\n",
       " 'evil': 916,\n",
       " 'en': 917,\n",
       " 'rolling': 918,\n",
       " 'special': 919,\n",
       " 'welcome': 920,\n",
       " 'freedom': 921,\n",
       " 'sitting': 922,\n",
       " 'dreaming': 923,\n",
       " \"tryin'\": 924,\n",
       " '2x': 925,\n",
       " 'learned': 926,\n",
       " 'treat': 927,\n",
       " 'sugar': 928,\n",
       " 'glad': 929,\n",
       " 'seven': 930,\n",
       " 'leaves': 931,\n",
       " \"fuckin'\": 932,\n",
       " 'clouds': 933,\n",
       " 'record': 934,\n",
       " 'ball': 935,\n",
       " 'brown': 936,\n",
       " 'shadow': 937,\n",
       " 'bottom': 938,\n",
       " 'case': 939,\n",
       " 'holy': 940,\n",
       " 'un': 941,\n",
       " 'sleeping': 942,\n",
       " 'rather': 943,\n",
       " 'screaming': 944,\n",
       " 'small': 945,\n",
       " 'jack': 946,\n",
       " 'neck': 947,\n",
       " 'fresh': 948,\n",
       " 'fighting': 949,\n",
       " 'trees': 950,\n",
       " \"feelin'\": 951,\n",
       " 'dirt': 952,\n",
       " 'slowly': 953,\n",
       " 'means': 954,\n",
       " 'mirror': 955,\n",
       " 'fate': 956,\n",
       " 'tv': 957,\n",
       " 'gangsta': 958,\n",
       " 'shadows': 959,\n",
       " 'price': 960,\n",
       " 'anybody': 961,\n",
       " 'women': 962,\n",
       " 'fucked': 963,\n",
       " 'i’ll': 964,\n",
       " 'needs': 965,\n",
       " 'taken': 966,\n",
       " 'em': 967,\n",
       " 'paid': 968,\n",
       " 'couple': 969,\n",
       " 'somehow': 970,\n",
       " \"'round\": 971,\n",
       " 'sand': 972,\n",
       " 'instead': 973,\n",
       " 'tu': 974,\n",
       " \"lovin'\": 975,\n",
       " 'weak': 976,\n",
       " 'hour': 977,\n",
       " 'job': 978,\n",
       " 'feed': 979,\n",
       " 'scene': 980,\n",
       " 'christmas': 981,\n",
       " 'joy': 982,\n",
       " 'later': 983,\n",
       " 'turns': 984,\n",
       " 'lock': 985,\n",
       " 'kept': 986,\n",
       " 'edge': 987,\n",
       " 'murder': 988,\n",
       " 'dig': 989,\n",
       " 'pimp': 990,\n",
       " 'shoulder': 991,\n",
       " 'held': 992,\n",
       " 'awake': 993,\n",
       " 'tree': 994,\n",
       " 'locked': 995,\n",
       " 'rich': 996,\n",
       " 'wings': 997,\n",
       " 'snow': 998,\n",
       " 'rules': 999,\n",
       " \"haven't\": 1000,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 59470\n",
    "oov_tok = \"<OOV>\"\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token= oov_tok)\n",
    "tokenizer.fit_on_texts(lyric_data_train)\n",
    "word_index = tokenizer.word_index\n",
    "display(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091beef",
   "metadata": {},
   "source": [
    "In order to be able to train our model using the available texts, we then use the `texts_to_sequences` method to convert our text to a sequence of integers. This is done by using the most frequent words in our `word_index`, and replacing these words with their respective tokens (which are integers) such that the model that we will build is able to interpret the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aad4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_data_train_tokenized = tokenizer.texts_to_sequences(lyric_data_train)\n",
    "lyric_data_test_tokenized = tokenizer.texts_to_sequences(lyric_data_test)\n",
    "lyric_data_val_tokenized = tokenizer.texts_to_sequences(lyric_data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde139ea",
   "metadata": {},
   "source": [
    "The final step we take in tokenizing our data is padding or truncating all of our sequences, such that they have the same length. This is done, as all neural networks require that inputs have the same size. A popular method of defining a length for which to pad or truncate our sequences to is the `max_length`, however, in this notebook, we instead consider the most frequent length of a sequence and the median length of a sequences. To start, we look at the most frequent length of a sequence. We do this by defining a new list, `len_lyric_set_flattened` and appending on the length of each song to the list. We then loop through the `len_lyric_set_flattened` list and use the `count()` method to see how many times an element of the specified length occurs in the `len_lyric_set_flattened` list and comparing it the `max_lyric_len_count`. If it occurs more frequently, we set the `max_lyric_len_count` to the count we recieved and the `max_lyric_len` to this length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28017527",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lyric_len = 0\n",
    "max_lyric_len_count = 0\n",
    "len_lyric_set_flattened = []\n",
    "\n",
    "for lyric in lyric_set_flattened:\n",
    "    len_lyric_set_flattened.append(len(lyric))\n",
    "\n",
    "for full_lyric_len in len_lyric_set_flattened:\n",
    "    full_lyric_len_frequency = len_lyric_set_flattened.count(full_lyric_len)\n",
    "    if full_lyric_len_frequency > max_lyric_len_count:\n",
    "        max_lyric_len_count = full_lyric_len_frequency\n",
    "        max_lyric_len = full_lyric_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba36a1c",
   "metadata": {},
   "source": [
    "By doing so, we see that the most frequent song length is 14 words, which occured 493 times. However, given that there are ~25k songs in our corpus, this is not a great metric to base the length of our padding length to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9195e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most frequent song length: 14\n",
      "count of the most frequent song length: 493\n"
     ]
    }
   ],
   "source": [
    "print(\"most frequent song length:\", max_lyric_len)\n",
    "print(\"count of the most frequent song length:\", max_lyric_len_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c53c2",
   "metadata": {},
   "source": [
    "Instead, we use the median of these lengths, to get a better understanding of which length most songs are around. We do this by first checking if the number of songs we have is odd or even, and then taking the average of the middle two elements or the middle element, respectively. Accordingly, we see that the median length of our songs is 1114 words, which looking at our histogram, makes more sense, as there is a large number of songs that occur in that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d948d798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median song length: 1114.0\n"
     ]
    }
   ],
   "source": [
    "len_lyric_set_flattened.sort()\n",
    "length_lyric_len_set = len(len_lyric_set_flattened)\n",
    "mid_length = length_lyric_len_set // 2\n",
    "median_length = 0\n",
    "\n",
    "if length_lyric_len_set % 2 == 0:\n",
    "    medR = len_lyric_set_flattened[mid_length]\n",
    "    medL = len_lyric_set_flattened[mid_length - 1]\n",
    "    median_length = (medR + medL) / 2\n",
    "else:\n",
    "    median_length = len_lyric_set_flattened[mid_length]\n",
    "\n",
    "print(\"median song length:\", median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ce78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_lyrics = []\n",
    "for lyric in lyric_set_flattened:\n",
    "    len_lyrics.append(len(lyric))\n",
    "\n",
    "plt.hist(len_lyrics, bins='auto')\n",
    "plt.title(\"Lengths of Songs in the Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e10de",
   "metadata": {},
   "source": [
    "We then pad or truncate the sequences using the `pad_sequences` method, which takes the sequenced text that we just defined, and pads all of the sequences or truncates them to a length of 1140, by adding on 0s until the sequence has a length of 150, or removing integers from the sequence, until the sequence has a length of 1140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81134494",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_length = int(median_length)\n",
    "lyric_data_train_padded = pad_sequences(lyric_data_train_tokenized, maxlen = median_length, \n",
    "                                        padding = 'post', truncating = 'post')\n",
    "lyric_data_test_padded = pad_sequences(lyric_data_test_tokenized, maxlen = median_length, \n",
    "                                        padding = 'post', truncating = 'post')\n",
    "lyric_data_val_padded = pad_sequences(lyric_data_val_tokenized, maxlen = median_length, \n",
    "                                        padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9a960",
   "metadata": {},
   "source": [
    "The final step in the pre-processing of our text data is combining the training, testing, and validation sets into a single list `lyric_set_flattened_padded`, so that we can export our pre-processed data for later use. We do this by defining an empty list, `lyric_set_flattened_padded` and then using the `extend()` method to add our training, testing, and validation sets to the end of this list, as opposed to the `append()` method which would add our training, testing, and validation sets as seperate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04119a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_set_flattened_padded = []\n",
    "lyric_set_flattened_padded.extend(lyric_data_train_padded)\n",
    "lyric_set_flattened_padded.extend(lyric_data_test_padded)\n",
    "lyric_set_flattened_padded.extend(lyric_data_val_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dd219",
   "metadata": {},
   "source": [
    "Finally, we create a new list, `full_set`, which contains our text and image data, for exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa73f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = [lyric_set_flattened_padded, pixels_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ee62e",
   "metadata": {},
   "source": [
    "We export our data using `pickle` which allows us to export our python object as a byte stream and store it on our disk. To do so, we specify that we will be writing a file via the `w` in a byte stream representation, via the `b`. We then open the `dataset.pkl` file for writing and dump the contents we want into our pickle file, where `full_set` is our content and `f` being the file that we will save to our disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ab64562",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(full_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07573a",
   "metadata": {},
   "source": [
    "# NOT FOR ARTIFACT - EXTRA WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b35bda",
   "metadata": {},
   "source": [
    "In the pre-processing of our image data, the last step we take is resizing our images, such that they are `120x120` isntead of `300x300`. We do this useing `cv2.resize()`, which takes in several parameters: the image we want to reshape as the `src` parameter and the dimensions of the reshaped image as a tuple for the `dsize` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cffe089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pct = 40\n",
    "for img_resize in pixels_set:\n",
    "    try:\n",
    "        wdth = int(img_resize.shape[0] * scale_pct / 100)\n",
    "        hgt = int(img_resize.shape[1] * scale_pct / 100)\n",
    "        dim = (wdth, hgt)\n",
    "        img_resize = cv2.resize(img_resize, dim)\n",
    "        # I have not used imwrite() yet, as based on the model we are using, it may or may not be necessary to reshape the data\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
